[
["index.html", "Data Analytics with R Preface", " Data Analytics with R Adam Smith, UCL School of Management November 07 2019 Preface This book is written for use in MSIN0010: Data Analytics I at the UCL School of Management. It is meant to serve as a supplement to lecture and seminar materials and specifically focuses on applications in R. "],
["introduction-to-data-analytics.html", "1 Introduction to Data Analytics 1.1 Loading and Inspecting Data Sets 1.2 Statistical Summaries 1.3 Graphical Summaries", " 1 Introduction to Data Analytics 1.1 Loading and Inspecting Data Sets The following line of code reads a data set that contains weekly prices, promotional activity, and sales for 20 different brands of beer. The data come from a Chicago grocery retailer (Dominick’s Finer Foods) and cover more than five years of transactions. The data set is publically available from the Kilts Center for Marketing at the University of Chicago.1 beer = read.csv(&quot;beer.csv&quot;, sep=&quot;,&quot;) Notice we used the sep= option to specify the delimiter used in our data file (which is a comma , for this data file). We always want to view the data after importing to make sure all the values were read-in correctly. To inspect the first few lines of a data set, use the head( ) command. head(beer) store upc week move price sale profit brand packsize itemsize units 86 1.82e+09 91 23 3.49 19.05 BUDWEISER BEER 6 12 oz 86 1.82e+09 91 9 3.79 28.23 O’DOUL’S N/A LONGNEC 6 12 oz 86 1.82e+09 91 9 3.69 22.03 BUDWEISER BEER LONG 6 12 oz 86 1.82e+09 91 78 3.29 B 5.78 MICHELOB REGULAR BEE 6 12 oz 86 3.41e+09 91 35 3.69 22.98 MILLER LITE BEER 6 12 oz 86 3.41e+09 91 12 3.69 22.98 MILLER GENUINE DRAFT 6 12 oz We can see that our data set contains 11 different variables (i.e., columns). A brief summary of each variable is provided below. store: unique store ID number upc: Universal Product Code week: week ID number move: number of units sold price: retail price in US dollars sale: indicator of promotional activity profit: gross profit margin brand: brand name packsize: number of items in one package itemsize: size of items in one package units: units of items Finally, we need to attach the data set so that R knows where to find the variables we want to analyze (e.g., move, price). attach(beer) Now we can access/view any of the variables by typing their name. For example, let’s look at the first 5 observations of the price variable. price[1:5] ## [1] 3.49 3.79 3.69 3.29 3.69 Here the closed brackets [ ] allow us to subset the data and the numbers 1:5 tell R to return observations 1, 2, 3, 4, and 5. 1.2 Statistical Summaries We can perform a variety of statistical summaries on our data using R’s built-in functions. A summary of commonly used statistical functions is provided below. Statistic R Function mean mean( ) median median( ) variance var( ) standard deviation sd( ) correlation cor( ) For example, we can compute the average demand (i.e., units sold) variable using the mean( ) function. mean(move) ## [1] 18.81 Now suppose we wanted to find the average demand only for a certain brand of beer, say Budweiser. To do this, we first need to subset move and only include observations where brand is equal to BUDWEISER BEER. move[brand==&quot;BUDWEISER BEER&quot;] Again, using closed brackets [ ] allow us to subset the data. We are interested in the move variable, but now we only want the observations where the condition stated inside the brackets is satisfied. Note: a double equals sign == is always used when writing logical statements to check equality. Now we can use the mean( ) function to compute the desired average demand for Budweiser. mean(move[brand==&quot;BUDWEISER BEER&quot;]) ## [1] 16.33 An all-purpose way to compute averages across different factor levels is to use the aggregate( ) function. The first argument specifies the variable we want to average and associated categorical/factor variable, and the second argument specifies the function we want to execute (i.e., the mean). aggregate(move ~ brand, FUN=mean) brand move BECK’S REG BEER NR B 18.35 BERGHOFF REGULAR BEE 15.57 BUDWEISER BEER 16.33 BUDWEISER BEER LONG 18.24 CORONA EXTRA BEER NR 15.39 HEINEKEN BEER N.R.BT 16.74 LOWENBRAU BEER NR BT 16.89 MICHELOB REGULAR BEE 14.23 MILLER GEN DRFT LNNR 50.96 MILLER GEN DRFT LT L 20.06 MILLER GENUINE DRAFT 16.37 MILLER HIGH LIFE LNN 14.05 MILLER LITE BEER 18.09 MILLER LITE BEER N.R 18.71 MILLER LITE LONGNECK 38.39 MILLER SHARP’S N/A L 11.45 O’DOUL’S N/A LONGNEC 11.96 OLD STYLE BEER 13.38 SAMUEL ADAMS LAGER N 20.62 1.3 Graphical Summaries Creating visual data summaries in R is also one of its strengths. Examples of useful graphical functions include hist( ), boxplot( ), plot( ), pie( ), barplot( ). An example of a histogram of weekly demand is shown below. hist(move, main=&quot;Distribution of Beer Sales Volume&quot;) Here the main= option allows us to customize the plot title. We can also use the breaks= option to control the bin width. hist(move, main=&quot;Distribution of Beer Sales Volume&quot;, breaks=50) We can also use a boxplot to look at the distribution of balance. To do this, we’ll use the boxplot( ) function. boxplot(move, main=&quot;Distribution of Beer Sales Volume&quot;) Notice how the extreme positive values (outliers) make the boxplot hard to analyze. We can remove these outliers with the outline=FALSE option. boxplot(move, main=&quot;Distribution of Beer Sales Volume&quot;, outline=FALSE) We can also use the boxplot( ) function to see how the distribution of demand changes across brands of beer. boxplot(move ~ brand, main=&quot;Distribution of Beer Sales Volume&quot;, outline=FALSE, xlab=&quot;&quot;, ylab=&quot;weekly units sold&quot;) The first argument tells R that we want to plot move against brand. Note that the numeric variable (e.g., move) must always come before the factor variable (e.g., brand). We’ve also added labels to the x- and y-axes using the xlab= and ylab= options. One issue with the plot above is that the labels on the x-axis are too wide. To solve this problem, we can rotate the x-axis labels by adding las=2 as an option in the boxplot( ) command. We will also want to slightly adjust the plot margins to allow for more space below the graph. This can be done by adding par(mar=c(a,b,c,d)) before the boxplot( ) command, where a,b,c,d denote the bottom, left, top and right margins, respectively. Lastly, we can use the cex.axis option to adjust the axis label sizes. par(mar=c(10.1,4.1,4.1,2.1)) boxplot(move ~ brand, main=&quot;Distribution of Beer Sales Volume&quot;, outline=FALSE, xlab=&quot;&quot;, ylab=&quot;weekly units sold&quot;, las=2, cex.axis=.75) We can create scatter plots using the plot( ) function. plot(price, move, main=&quot;Beer Demand Patterns&quot;) The first argument corresponds to the variable on the x-axis, while the second corresponds to the variable on the y-axis. To modify the type or color of the points in the above plot, we can add pch and col options. For example, let’s change the points to be closed circles (pch=16) that are red (col=2). plot(price, move, main=&quot;Beer Demand Patterns&quot;, pch=16, col=2) Often times, adding color is a great way to incorporate information about other variables. For example, should the relationship between price and demand change by brand? The answer is likely yes. To see this visually, let’s change the color of each point to represent the corresponding brand. palette = rainbow(20) colors = palette[as.numeric(as.factor(brand))] plot(price, move, main=&quot;Beer Demand Patterns&quot;, pch=16, col=colors) legend(&quot;topright&quot;, legend=sort(unique(brand)), pch=16, col=colors, cex=.5, bty=&quot;n&quot;) The first line of code generates 20 unique colors from a rainbow palette, one for each brand. Because there are hundreds of points on our scatter plot, we need to identify assign a color to each point. This is what the second line of code does. In the third line, we simply use the colors object we just created to specify point colors. The last line of code adds a legend to the plot so that we know what the colors correspond to. The first argument indicates the legend position, the second argument legend indicates the values to be shown on the legend, the third argument pch is the point type, the fourth argument col is the color for each value, and the fifth argument cex adjusts the text size. https://www.chicagobooth.edu/research/kilts/datasets/dominicks↩ "],
["computing-probabilities.html", "2 Computing Probabilities 2.1 Normal Distribution 2.2 Bernoulli and Binomial Distributions", " 2 Computing Probabilities There are many common families of probability distributions and we have discussed six so far. The discrete distributions include the discrete Uniform, Bernoulli, and Binomial. The continuous distributions include the continuous Uniform, Normal, and t. This chapter provides a set of examples to show you how to compute probabilities from a few of these distributions in R. 2.1 Normal Distribution R has four normal distribution functions: dnorm( ), pnorm( ), qnorm( ), and rnorm( ). dnorm(x,mean,sd) probability density function (PDF) - input: x is the value at which you want to evaluate the normal PDF - output: a positive number since the PDF \\(f(x)\\) must be positive - example: evaluate \\(f(x)\\) pnorm(q,mean,sd) cumulative distribution function (CDF) - input: q is the value for which you want to find the area below/above - output: a probability - example: compute \\(P(X&lt;q)\\) qnorm(p,mean,sd) quantile function - input: p is a probability - output: a real number since \\(X\\in(-\\infty,\\infty)\\) - example: find the value \\(q\\) such that \\(P(X&lt;q)=p\\) rnorm(n,mean,sd) random number generator - input: n is the number of observations you want to generate - output: a vector of n real numbers - example: generate n independent \\(N(\\mu,\\sigma^2)\\) random variables More information is also accessible in R if you type ?dnorm, ?pnorm, ?qnorm, or ?rnorm. To learn how to use these functions, we’ll start with a few exercises on the standard normal distribution which is a normal distribution with mean 0 and standard deviation of 1. We will then move on to the more general \\(N(\\mu,\\sigma^2)\\) distribution. 2.1.1 Probability Density Function (dnorm) When \\(X\\) is a continuous random variable, we know that \\(P(X=x)=0\\). Therefore, dnorm( ) does not return a probability, but rather the height of the PDF. Even though the height of the PDF is not a probability, we can still interpret density evaluations as the relatively likelihood of observing a certain value \\(x\\). PROBLEM 1: Let \\(X\\sim N(0,1)\\). Is the value \\(x=1\\) or \\(x=-0.5\\) more likely to occur under this normal distribution? dnorm(1, mean=0, sd=1) ## [1] 0.242 dnorm(-0.5, mean=0, sd=1) ## [1] 0.3521 The results show that \\(x=-0.5\\) is more likely, since \\(f(-0.5)&gt;f(1)\\). This should be expected because we know that density function is symmetric and peaks at the mean value which is 0 here. Since \\(x=-0.5\\) is closer to 0 than \\(x=1\\), it should have higher likelihood under \\(N(0,1)\\) distribution. 2.1.2 Cumulative Distribution Function (pnorm) The pnorm( ) function is useful for evaluating probabilities of the form \\(P(X\\leq x)\\) or \\(P(X \\geq x)\\). PROBLEM 2: If \\(X\\sim N(0,1)\\), what is \\(P(X&lt;0)\\)? pnorm(0, mean=0, sd=1) ## [1] 0.5 PROBLEM 3: If \\(X\\sim N(0,1)\\), what is \\(P(X&lt;1)\\)? pnorm(1, mean=0, sd=1) ## [1] 0.8413 PROBLEM 4: If \\(X\\sim N(0,1)\\), what is \\(P(X&gt;1)\\)? We have two ways of answering this question. First, we can recognize that \\(P(X&gt;1)=1-P(X\\geq 1)\\). 1-pnorm(1, mean=0, sd=1) ## [1] 0.1587 A second approach is to use the lower.tail= option within the pnorm( ) function. When lower.tail=TRUE then the pnorm( ) function returns the probability to the left of a given number \\(x\\) and if lower.tail=FALSE then pnorm( ) returns the probability to the right of \\(x\\). pnorm(1, mean=0, sd=1, lower.tail=FALSE) ## [1] 0.1587 PROBLEM 5: If \\(X\\sim N(0,1)\\), what is \\(P(0&lt;X&lt;1)\\) pnorm(1, mean=0, sd=1) - pnorm(0, mean=0, sd=1) ## [1] 0.3413 Once we understand how to use the pnorm( ) function to compute standard normal probabilities, extending the function to compute probabilities of any normal distribution is straightforward. All we have to do is change the mean= and sd= arguments. Remember that the normal functions in R call for the standard deviation \\(\\sigma\\), NOT the variance \\(\\sigma^2\\)! PROBLEM 6: If \\(X\\sim N(4,9)\\), what is \\(P(X&lt;0)\\)? pnorm(0, mean=4, sd=3) ## [1] 0.09121 PROBLEM 7: If \\(X\\sim N(2,3)\\), what is \\(P(X&gt;5)\\)? pnorm(5, mean=2, sd=sqrt(3), lower.tail=FALSE) ## [1] 0.04163 2.1.3 Quantile Function (qnorm) Next, let’s use the qnorm( ) function to find quantiles of the normal distribution. PROBLEM 8: If \\(X\\sim N(0,1)\\), find the value \\(q\\) such that \\(P(X&lt;q)=0.05\\). qnorm(0.05, mean=0, sd=1) ## [1] -1.645 PROBLEM 9: If \\(X\\sim N(0,1)\\), find the value \\(q\\) such that \\(P(X&gt;q)=0.025\\). That is, \\(q\\) is the value such that 2.5% of the area under the standard normal PDF is to its right. qnorm(0.025, mean=0, sd=1, lower.tail=FALSE) ## [1] 1.96 PROBLEM 10: If \\(X\\sim N(-4,2)\\), find the value \\(q\\) such that \\(P(X&gt;q)=0.1\\). That is, \\(q\\) is the value such that 10% of the area under the \\(N(-4,2)\\) PDF is to its right. qnorm(0.1, mean=-4, sd=sqrt(2), lower.tail=FALSE) ## [1] -2.188 2.1.4 Random Number Generator (rnorm) Finally, let’s use rnorm( ) to generate random samples of size \\(n\\) from a normal distribution. PROBLEM 11: Generate \\(n=20\\) random variables from a standard normal distribution. x = rnorm(20, mean=0, sd=1) x ## [1] -0.62645 0.18364 -0.83563 1.59528 0.32951 -0.82047 0.48743 ## [8] 0.73832 0.57578 -0.30539 1.51178 0.38984 -0.62124 -2.21470 ## [15] 1.12493 -0.04493 -0.01619 0.94384 0.82122 0.59390 hist(x) PROBLEM 12: Generate \\(n=100\\) random variables from a \\(N(10,2)\\) distribution. x = rnorm(100, mean=10, sd=sqrt(2)) x ## [1] 11.300 11.106 10.105 7.187 10.877 9.921 9.780 7.920 9.324 10.591 ## [11] 11.921 9.855 10.548 9.924 8.053 9.413 9.442 9.916 11.556 11.079 ## [21] 9.767 9.642 10.986 10.787 9.026 8.999 10.516 11.087 9.841 11.246 ## [31] 10.563 9.134 10.482 8.403 12.027 12.801 9.481 8.523 10.806 9.809 ## [41] 13.396 9.945 10.975 10.040 8.949 10.267 7.447 12.073 10.217 13.073 ## [51] 10.672 8.996 10.864 8.679 8.227 10.412 9.373 10.002 10.105 9.166 ## [61] 9.196 9.809 11.666 7.845 10.840 10.471 11.503 9.570 10.523 10.378 ## [71] 9.233 11.708 11.641 10.990 12.244 10.790 8.195 9.189 8.268 9.331 ## [81] 9.123 10.060 8.712 10.223 9.074 12.499 11.014 11.287 10.543 12.379 ## [91] 9.101 9.347 12.026 9.080 9.707 9.444 9.547 9.605 10.699 9.749 hist(x) 2.2 Bernoulli and Binomial Distributions The Bernoulli and Binomial distributions are intimately related: a Binomial random variable corresponds to the number of successes in \\(n\\) independent Bernoulli trials. For example, consider flipping a coin. Each coin flip can be modelled as a Bernoulli\\((p)\\) random variable with probability of success (heads) equal to \\(p\\). If you flipped a coin \\(n=10\\) times and wanted to model the number of sucesses (heads) in \\(n=10\\) trials, that would be a Binomial(\\(n,p\\)) random variable. R has four functions that can be used to compute both Bernoulli and Binomial probabilities: dbinom( ), pbinom( ), qbinom( ), rbinom( ). dbinom(x,size,prob) probability mass function (PMF) - input: x is the number of successes, size is the number of trials \\(n\\), prob is the probability of success \\(p\\) - output: a probability since \\(0\\leq P(X=x)\\leq1\\) - example: evaluate \\(P(X=x)\\) pbinom(q,size,prob) probability distribution function (CDF) - input: q is the value for which you want to find the area below/above, size is the number of trials \\(n\\), prob is the probability of success \\(p\\) - output: a probability - example: evaluate \\(P(X\\leq x)\\) qbinom(p,size,prob) quantile function - input: p is a probability, size is the number of trials \\(n\\), prob is the probability of success \\(p\\) - output: a positive integer since \\(X\\in\\{0,1,\\dotsc,n\\}\\) - example: find \\(q\\) s.t. \\(P(X\\leq q)=p\\) rbinom(n,size,prob) random number generator - input: n is the number of observations you want to generate, size is the number of trials \\(n\\), prob is the probability of success \\(p\\) - output: a vector of n positive integers - example: generate \\(n\\) independent Binomial\\((n,p)\\) random variables Note: To use these functions to compute Bernoulli probabilities, set size=1. More information is also accessible in R if you type ?dbinom, ?pbinom, ?qbinom, or ?rbinom. 2.2.1 Probability Mass Function (dbinom) PROBLEM 1: If you flip a coin \\(n=5\\) times and in each flip the probability of heads is \\(p=0.5\\), what is the chance that you get 2 successes? Here, our random variable \\(X\\) is the number of successes in \\(n\\) independent trials, so \\(X\\sim\\text{Binomial}(n,p)\\) with \\(n=5\\) and \\(p=0.5\\). dbinom(2, size=5, prob=0.5) ## [1] 0.3125 We can also check our answer using the Binomial probability mass function: \\(P(X=x)={n\\choose x}p^x(1-p)^{n-x}\\). choose(5,2)*0.5^2*(1-0.5)^(5-2) ## [1] 0.3125 2.2.2 Cumulative Distribution Function (pbinom) PROBLEM 2: If you flip a coin \\(n=5\\) times and in each flip the probability of heads is \\(p=0.5\\), what is the chance that you get at most 2 successes? Now we want to find \\(P(X\\leq2)\\). We know that \\(P(X\\leq2)=P(X=2)+P(X=1)+P(X=0)\\), so we could again use the dbinom( ) function. dbinom(2, size=5, prob=0.5) + dbinom(1, size=5, prob=0.5) + dbinom(0, size=5, prob=0.5) ## [1] 0.5 The problem is that this approach becomes cumbersome as the number of trials increases. A more efficient approach is to recognize that \\(P(X\\leq2)\\) takes the form of the CDF and use pnorm( ). pbinom(2, size=5, prob=0.5) ## [1] 0.5 PROBLEM 3: If you flip a coin \\(n=100\\) times and in each flip the probability of heads is \\(p=0.25\\), what is the chance that you get at most 20 successes? pbinom(20, size=100, prob=0.25) ## [1] 0.1488 PROBLEM 4: If you flip a coin \\(n=100\\) times and in each flip the probability of heads is \\(p=0.25\\), what is the chance that you get at least 20 successes? We have two ways to solve this problem. First, we can write \\(P(X\\geq 20)=1-P(X&lt;20)=1-P(X\\leq 19)\\) where \\(P(X&lt;20)=P(X\\leq 19)\\) since \\(X\\) is discrete. 1-pbinom(19, size=100, prob=0.25) ## [1] 0.9005 Alternatively, we can use the “lower.tail=” option to tell R we want the probability greater than x. However, note that this is strictly greater than, so we must again remember than \\(P(X\\geq 20)=P(X&gt;19)\\). pbinom(19, size=100, prob=0.25, lower.tail=FALSE) ## [1] 0.9005 2.2.3 Quantile Function (qbinom) PROBLEM 5: Suppose you flip a coin \\(n=20\\) times where each flip has a probability of heads equal to \\(p=0.5\\). Find the value \\(q\\) such that the probability of getting at most \\(q\\) successes is equal to 0.25. qbinom(0.25, size=20, prob=0.5) ## [1] 8 2.2.4 Random Number Generator (rbinom) PROBLEM 5: Generate \\(n=50\\) Bernoulli\\((p)\\) random variables with \\(p=0.2\\). x = rbinom(50, size=1, prob=0.2) x ## [1] 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [36] 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 barplot(table(x)) PROBLEM 6: Generate \\(n=100\\) Binomial\\((n,p)\\) random variables with \\(p=0.4\\). x = rbinom(100, size=100, prob=0.2) x ## [1] 17 16 29 18 20 22 15 15 14 26 22 15 20 20 19 30 16 24 14 19 16 17 24 ## [24] 22 17 20 15 18 28 21 22 18 19 31 24 27 24 23 17 23 29 18 19 24 14 19 ## [47] 19 16 21 28 30 16 20 19 22 17 20 16 19 22 20 14 23 19 29 18 24 15 25 ## [70] 20 15 18 24 18 21 24 14 22 22 20 19 21 26 17 17 19 18 24 16 20 19 21 ## [93] 22 28 17 17 23 24 15 27 barplot(table(x)) "],
["sampling-distributions-and-the-clt.html", "3 Sampling Distributions and the CLT", " 3 Sampling Distributions and the CLT Sampling distributions are theoretical objects that represent the probability distribution of a statistic (usually the sample mean). The sampling distribution of the sample mean is the distribution of means that result from taking all possible samples of size \\(n\\) from the population. We can build some intuition for what this means in R. Let’s start by letting the population distribution be normal with mean \\(\\mu=0\\) and variance \\(\\sigma^2=1\\). We can take a sample of size \\(n=100\\) from that population and plot the sample distribution. x = rnorm(n=100, mean=0, sd=1) mean(x) ## [1] 0.008768 hist(x, main=&quot;Sample Distribution&quot;) While we cannot generate all possible samples of size \\(n\\) from this normal distribution, we can take a large number. Let \\(R=100,000\\) be the number of samples we want to generate. We will construct a for loop in R to do the following: (1) generate a random sample of size \\(n\\) from the population; (2) compute the sample mean and store the result in a vector called xbar. R = 100000 xbar = double(R) for(r in 1:R){ x = rnorm(n=100, mean=0, sd=1) xbar[r] = mean(x) } hist(xbar, main=&quot;Sampling Distribution of Xbar&quot;) This plot is the distribution of sample means after taking \\(R=100,000\\) samples with size \\(n=100\\) from the population. Notice that this again looks like a normal distribution. The mean looks the same as the distribution of \\(X\\), but the variance looks much smaller. In fact, when the population distribution is \\(N(\\mu,\\sigma^2)\\), then the distribution of \\(\\bar{X}\\) is \\(N(\\mu,\\sigma^2/n)\\). In this case, we know that \\(E(\\bar{X})=E(X)=\\mu=0\\) and \\(Var(\\bar{X})=Var(X)/n=\\sigma^2/n=1/100=0.01\\). Let’s check to see what the mean and variance are in our approximation of the sampling distribution. mean(xbar) ## [1] 0.0004032 var(xbar) ## [1] 0.009978 Very close to the true values! The next question to consider is what if our population distribution was not normally distributed? What if it was skewed to the right or left? Let’s assume \\(X\\) has a Exponential\\((\\beta)\\) distribution where \\(\\beta=1\\) is a rate parameter. We can again take one individual sample from this population distribution. x = rexp(n=100, rate=1) hist(x, main=&quot;Sample Distribution&quot;) Based on this sample, it appears that the population distribution is highly asymmetric and skewed to the right. In this case, should we still expect the sampling distribution of the sample mean to be normal? Let’s go through the same exercise as before. R = 100000 xbar = double(R) for(r in 1:R){ x = rexp(n=100, rate=1) xbar[r] = mean(x) } hist(xbar, main=&quot;Sampling Distribution of Xbar&quot;) The distribution of sample means again looks normal! In fact, the Central Limit Theorem guarantees that this will be the case as the sample size \\(n\\) gets large. Then for any population distribution, we know that the distribution of \\(\\bar{X}\\) will be approximately normal with mean \\(E(\\bar{X})=E(X)\\) and \\(Var(\\bar{X})=Var(X)/n\\). Since \\(X\\sim\\text{Exponential}(\\beta)\\) with \\(\\beta=1\\), it can be shown that \\(E(X)=\\beta=1\\) and \\(Var(X)=\\beta^2=1\\). Therefore, \\(\\bar{X}\\) is approximately normal with mean \\(E(\\bar{X})=E(X)=1\\) and variance \\(Var(\\bar{X})=Var(X)/n=1/100=0.01\\). We again check these results using our approximate sampling distribution and find consistent answers. mean(xbar) ## [1] 1 var(xbar) ## [1] 0.01004 "],
["estimation.html", "4 Estimation 4.1 Point Estimation 4.2 Confidence Intervals", " 4 Estimation 4.1 Point Estimation Point estimation is all about using one single number (statistic) to estimate a population parameter of interest. One question is how do we actually come up with those statistics? In this section, we will illustrate the concept of maximum likelihood estimation. The idea of maximum likelihood estimation is to first assume our data come from a known family of distributions (e.g., normal) that contain parameters (e.g., mean \\(\\mu\\), variance \\(\\sigma^2\\)). Then the maximum likelihood estimates (MLEs) of the parameters will be the parameter values that are most likely to have generated our data, where “most likely” is measured by the likelihood function. To start, let’s create a simple data set. We will generate \\(n=25\\) normal random variables with mean \\(\\mu=5\\) and variance \\(\\sigma^2=1\\). x = rnorm(25, mean=5, sd=1) Now we will pretend as if we do not know the mean parameter of the population distribution \\(\\mu\\) and then use the method of maximum likelihood to estimate \\(\\mu\\). Remember that the MLE of \\(\\mu\\) is defined as \\(\\hat{\\mu}^{\\text{MLE}}=\\arg\\max f(x_1,\\dotsc,x_n|\\mu,\\sigma^2)\\). That is, \\(\\hat{\\mu}^{\\text{MLE}}\\) is the value of \\(\\mu\\) that maximizes the likelihood function. Once we have made the assumption that are our data are normally distributed, the likelihood function takes the form \\[f(x_1,\\dotsc,x_n|\\mu,\\sigma^2)=\\prod_{i=1}^n f(x_i|\\mu,\\sigma^2)=\\prod_{i=1}^n{1\\over\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-{1\\over2\\sigma^2}(x_i-\\mu)^2\\right)\\] where the right-most term is the PDF of the normal distribution. If we use calculus to formally maximize the likelihood function, we will find that \\(\\hat{\\mu}^{\\text{MLE}}=\\bar{X}\\). Since the MLE of \\(\\mu\\) is the sample mean, computing the MLE in R becomes straightforward. mean(x) ## [1] 5.162 Therefore, the MLE of \\(\\mu\\) is about 5.16. To help provide some intuition for how the maximum likelihood method works, let’s work through an example. Rather than try to work through the calculus to show that \\(\\hat{\\mu}^{\\text{MLE}}=\\bar{x}=5.16\\), we can plot the likelihood function against many candidate values of \\(\\mu\\) and see where the curve is highest. Since the likelihood function is a product of positive numbers (many of which can be very small), it’s more convenient and numerically stable to work with the log-likelihood function. \\[\\log f(x_1,\\dotsc,x_n|\\mu,\\sigma^2)=\\log\\left(\\prod_{i=1}^n f(x_i|\\mu,\\sigma^2)\\right)=\\sum_{i=1}^n \\log f(x_i|\\mu,\\sigma^2)\\] To compute the log-likelihood function for each \\(x_i\\), we can use the dnorm( ) function. Let’s start by evaluating the log-likelihood for each observation. To do this we need to plug in a value for the mean (which we will fix to its true value \\(\\mu=5\\)). dnorm(x, mean=5, sd=1, log=TRUE) ## [1] -1.2702 -1.0084 -0.9244 -1.4070 -2.8072 -0.9735 -2.2069 -1.6471 ## [9] -1.3350 -0.9204 -1.0977 -1.0532 -2.3580 -2.0379 -1.1200 -0.9289 ## [17] -1.0902 -1.4054 -1.5403 -0.9302 -2.1302 -1.5254 -1.9105 -2.3638 ## [25] -1.7857 The entire log-likelihood function is the sum of the individual log-likelihood evaluations. Therefore, the log-likelihood for \\(\\mu=5\\) can be found as follows. sum(dnorm(x, mean=5, sd=1, log=TRUE)) ## [1] -37.78 This number now represents a measure of the relative likelihood that our data x were generated from a normal distribution with mean 5. We can the imagine using the likelihood function as a way of finding “good” estimates of \\(\\mu\\). The “best” estimate of \\(\\mu\\) would correspond to the value that is most likely to have generated our data. As a simple example, we could say: is it more likely that our data come from a distribution with mean \\(\\mu=5\\) or \\(\\mu=6\\)? To test this, we could evaluate the log likelihood function for \\(\\mu=6\\) and compare. sum(dnorm(x, mean=6, sd=1, log=TRUE)) ## [1] -46.22 Given that the likelihood function is higher (less negative) for \\(\\mu=5\\), we say that \\(\\hat{\\mu}=5\\) is our estimate of \\(\\mu\\). Since \\(\\mu\\) can be any real number, there is no reason to restrict plausible values of \\(\\mu\\) to the set of integers. Instead, we can lay out a grid of candidate parameter values of any desired level of granularity. index = seq(3, 7, by=.01) index ## [1] 3.00 3.01 3.02 3.03 3.04 3.05 3.06 3.07 3.08 3.09 3.10 3.11 3.12 3.13 ## [15] 3.14 3.15 3.16 3.17 3.18 3.19 3.20 3.21 3.22 3.23 3.24 3.25 3.26 3.27 ## [29] 3.28 3.29 3.30 3.31 3.32 3.33 3.34 3.35 3.36 3.37 3.38 3.39 3.40 3.41 ## [43] 3.42 3.43 3.44 3.45 3.46 3.47 3.48 3.49 3.50 3.51 3.52 3.53 3.54 3.55 ## [57] 3.56 3.57 3.58 3.59 3.60 3.61 3.62 3.63 3.64 3.65 3.66 3.67 3.68 3.69 ## [71] 3.70 3.71 3.72 3.73 3.74 3.75 3.76 3.77 3.78 3.79 3.80 3.81 3.82 3.83 ## [85] 3.84 3.85 3.86 3.87 3.88 3.89 3.90 3.91 3.92 3.93 3.94 3.95 3.96 3.97 ## [99] 3.98 3.99 4.00 4.01 4.02 4.03 4.04 4.05 4.06 4.07 4.08 4.09 4.10 4.11 ## [113] 4.12 4.13 4.14 4.15 4.16 4.17 4.18 4.19 4.20 4.21 4.22 4.23 4.24 4.25 ## [127] 4.26 4.27 4.28 4.29 4.30 4.31 4.32 4.33 4.34 4.35 4.36 4.37 4.38 4.39 ## [141] 4.40 4.41 4.42 4.43 4.44 4.45 4.46 4.47 4.48 4.49 4.50 4.51 4.52 4.53 ## [155] 4.54 4.55 4.56 4.57 4.58 4.59 4.60 4.61 4.62 4.63 4.64 4.65 4.66 4.67 ## [169] 4.68 4.69 4.70 4.71 4.72 4.73 4.74 4.75 4.76 4.77 4.78 4.79 4.80 4.81 ## [183] 4.82 4.83 4.84 4.85 4.86 4.87 4.88 4.89 4.90 4.91 4.92 4.93 4.94 4.95 ## [197] 4.96 4.97 4.98 4.99 5.00 5.01 5.02 5.03 5.04 5.05 5.06 5.07 5.08 5.09 ## [211] 5.10 5.11 5.12 5.13 5.14 5.15 5.16 5.17 5.18 5.19 5.20 5.21 5.22 5.23 ## [225] 5.24 5.25 5.26 5.27 5.28 5.29 5.30 5.31 5.32 5.33 5.34 5.35 5.36 5.37 ## [239] 5.38 5.39 5.40 5.41 5.42 5.43 5.44 5.45 5.46 5.47 5.48 5.49 5.50 5.51 ## [253] 5.52 5.53 5.54 5.55 5.56 5.57 5.58 5.59 5.60 5.61 5.62 5.63 5.64 5.65 ## [267] 5.66 5.67 5.68 5.69 5.70 5.71 5.72 5.73 5.74 5.75 5.76 5.77 5.78 5.79 ## [281] 5.80 5.81 5.82 5.83 5.84 5.85 5.86 5.87 5.88 5.89 5.90 5.91 5.92 5.93 ## [295] 5.94 5.95 5.96 5.97 5.98 5.99 6.00 6.01 6.02 6.03 6.04 6.05 6.06 6.07 ## [309] 6.08 6.09 6.10 6.11 6.12 6.13 6.14 6.15 6.16 6.17 6.18 6.19 6.20 6.21 ## [323] 6.22 6.23 6.24 6.25 6.26 6.27 6.28 6.29 6.30 6.31 6.32 6.33 6.34 6.35 ## [337] 6.36 6.37 6.38 6.39 6.40 6.41 6.42 6.43 6.44 6.45 6.46 6.47 6.48 6.49 ## [351] 6.50 6.51 6.52 6.53 6.54 6.55 6.56 6.57 6.58 6.59 6.60 6.61 6.62 6.63 ## [365] 6.64 6.65 6.66 6.67 6.68 6.69 6.70 6.71 6.72 6.73 6.74 6.75 6.76 6.77 ## [379] 6.78 6.79 6.80 6.81 6.82 6.83 6.84 6.85 6.86 6.87 6.88 6.89 6.90 6.91 ## [393] 6.92 6.93 6.94 6.95 6.96 6.97 6.98 6.99 7.00 We now want to step through each value in the index and evaluate the log-likelihood function. The MLE will be the value in the index that yields the highest log-likelihood function. To do this, we will use a “for loop” in R. R = length(index) loglike = double(R) for(r in 1:R){ loglike[r] = sum(dnorm(x, mean=index[r], sd=1, log=TRUE)) } Here R counts the number of elements in the index variable and loglike is a vector of R elements that are initially set to be zero but are then filled in in each iteration of the for loop. plot(index, loglike, type=&quot;l&quot;) abline(v=index[which.max(loglike)], col=2, lty=2) Once we have evaluated the log-likelihood function for each value in the index, we can plot the log-likelihood against the index. Based on this plot, it looks like the value that maximizes the log-likehood functin is a little bit greater than 5. We formally check this in R. index[which.max(loglike)] ## [1] 5.16 This matches the MLE that we initially computed using the sample mean. mean(x) ## [1] 5.162 The only difference is the fact that our estimate using the plot is rounded to two decimal places because of the grid that we selected when creating the index variable. 4.2 Confidence Intervals Ultimately, point estimates are insufficient because the estimators themselves are random variables and subject to variation across different samples. Rather than provide a single point estimate, it will be better to provide a range of plausible values for the parameter of interest. This is the idea of confidence intervals. A confidence interval for any parameter \\(\\theta\\) will always take the form: \\[\\hat{\\theta}\\pm \\text{(critical value)}\\times SD(\\hat{\\theta})\\] where \\(\\hat{\\theta}\\) is an estimator of \\(\\theta\\), the critical value is a quantile of a normal or t distribution, and \\(SD(\\hat{\\theta})\\) is the standard deviation of the estimator. 4.2.1 Types of Confidence Intervals We will consider four types of \\((1-\\alpha)100\\%\\) confidence intervals. Confidence interval for mean \\(\\mu\\), data are normally distributed, variance \\(\\sigma^2\\) is known \\[\\bar{x}\\pm z_{\\alpha/2}{\\sigma\\over\\sqrt{n}}\\] Confidence interval for mean \\(\\mu\\), data are normally distributed, variance \\(\\sigma^2\\) is unknown \\[\\bar{x}\\pm t_{n-1,\\alpha/2}{s\\over\\sqrt{n}}\\] Confidence interval for mean \\(\\mu\\), data are not normally distributed \\[\\bar{x}\\pm t_{n-1,\\alpha/2}{s\\over\\sqrt{n}}\\] Confidence interval for proportion \\(p\\), data are binary (0s and 1s) \\[\\hat{p}\\pm z_{\\alpha/2}\\sqrt{\\hat{p}(1-\\hat{p})\\over n}\\] Notice that we can compute the critical values in R. If we need \\(z_{\\alpha/2}\\), then we must find the value of a standard normal random variable such that \\(\\alpha/2\\) percent of the area is to its right. This is can be found using the normal quantile function qnorm( )! For example, if \\(\\alpha=0.05\\) let’s use qnorm( ) to find \\(z_{0.025}\\). qnorm(0.025, mean=0, sd=1, lower.tail=FALSE) ## [1] 1.96 A summary of the most commonly used normal critical values are provided below. Confidence Level \\((1-\\alpha)\\) Critical Value \\(z_{\\alpha/2}\\) 99% 2.576 95% 1.96 90% 1.645 Similarly, if we need to compute \\(t_{n-1,\\alpha/2}\\) with \\(\\alpha=0.05\\) and our data have \\(n=50\\) observations, we can use the qt( ) function. qt(0.025, df=50-1, lower.tail=FALSE) ## [1] 2.01 Notice this critical value is larger than \\(z_{0.025}\\) – this comes from the fact that the t-distribution has fatter tails than the normal distribution. But, it also turns out that a t distribution (which only has one parameter \\(\\nu\\) called the “degrees of freedom”) converges to a normal distribution as \\(\\nu\\to\\infty\\). We can formally check this using qt( ). qt(0.025, df=50, lower.tail=FALSE) ## [1] 2.009 qt(0.025, df=100, lower.tail=FALSE) ## [1] 1.984 qt(0.025, df=1000, lower.tail=FALSE) ## [1] 1.962 qt(0.025, df=10000, lower.tail=FALSE) ## [1] 1.96 Notice how these values approach \\(z_{0.025}\\approx1.96\\) as the degrees of freedom parameter gets large. 4.2.2 Examples PROBLEM 1: A wine importer needs to report the average percentage of alcohol in bottles of French wine. From experience with previous kinds of wine, the importer believes that alcolhol percentages are normally distributed and the population standard deviation is 1.2%. The importer randomly samples 60 bottles of the new wine and obtains a sample mean \\(\\bar{x}=9.3\\%\\). Give a 90% confidence interval for the average percentage of alcohol in all bottles of the new wine. ANSWER: From the problem, we know the following. \\[\\begin{align*} n = 60\\\\ \\sigma=1.2\\\\ \\bar{x} =9.3\\\\ \\alpha=0.10 \\end{align*}\\] We must first figure out which type of confidence interval to use. Notice that we are trying to estimate the average percentage of alcohol, so our parameter is a mean \\(\\mu\\). Moreover, we are told to assume that the data are normally distributed and the population standard deviation \\(\\sigma\\) is known. Thefore, our confidence interval will be of the form: \\[\\bar{x}\\pm z_{\\alpha/2}{\\sigma\\over\\sqrt{n}}.\\] We can now define each object in R and construct the confidence interval. n = 60 sigma = 1.2 xbar = 9.3 zalpha = qnorm(0.05, mean=0, sd=1, lower.tail=FALSE) xbar - zalpha*sigma/sqrt(n) ## [1] 9.045 xbar + zalpha*sigma/sqrt(n) ## [1] 9.555 Therefore, we are 90% confident that the true average alcohol content in all new bottles of wine is between 9.05% and 9.55%. PROBLEM 2: An economist wants to estimate the average amount in checking accounts at banks in a given region. A random sample of 100 accounts gives \\(\\bar{x}=£357.60\\) and \\(s=£140.00\\). Give a 95% confidence interval for \\(\\mu\\), the average amount in any checking account at a bank in the given region. ANSWER: From the problem, we know the following. \\[\\begin{align*} n &amp;= 100\\\\ \\bar{x} &amp;=357.60\\\\ s &amp;= 140\\\\ \\alpha&amp;=0.5 \\end{align*}\\] Here we are not told whether the data are normally distributed. However, it won’t matter because we only have an estimate of \\(\\sigma\\) (remember that among the four types of confidence intervals we considered earlier, there are no differences between case II and III). Therefore, our confidence interval will be of the form: \\[\\bar{x}\\pm t_{n-1,\\alpha/2}{s\\over\\sqrt{n}}.\\] We can again define each object in R and construct the confidence interval. n = 100 xbar = 357.60 s = 140 talpha = qt(0.025, df=n-1, lower.tail=FALSE) xbar - talpha*s/sqrt(n) ## [1] 329.8 xbar + talpha*s/sqrt(n) ## [1] 385.4 Therefore, we are 95% confident that the true average account checking account value in the given region is between £329.82 and £385.38. PROBLEM 3: The EuStockMarkets data set in R provides daily closing prices of four major European stock indices: Germany DAX (Ibis), Switzerland SMI, France CAC, and UK FTSE. Using this data set, produce a 99% confidence interval for the average closing price of the UK FTSE. ANSWER: First, let’s load in the data from R. data(EuStockMarkets) head(EuStockMarkets) DAX SMI CAC FTSE 1629 1678 1773 2444 1614 1688 1750 2460 1607 1679 1718 2448 1621 1684 1708 2470 1618 1687 1723 2485 1611 1672 1714 2467 Now let’s pull the subset of data we care about (i.e., the UK FTSE column). uk = EuStockMarkets[,4] Notice that we are not told anything about the true distribution of the data. Therefore, our confidence interval will be of the form: \\[\\bar{x}\\pm t_{n-1,\\alpha/2}{s\\over\\sqrt{n}}.\\] Next, let’s compute each component necessary to construct the 99% confidence interval. n = length(uk) xbar = mean(uk) s = sd(uk) talpha = qt(0.005, df=n-1, lower.tail=FALSE) xbar - talpha*s/sqrt(n) ## [1] 3507 xbar + talpha*s/sqrt(n) ## [1] 3624 Therefore, we are 99% confident that the true closing price for the UK FTSE index is between £3,507.25 and £3,624.04. Finally, notice that we can use the t.test( ) function to perform the same analysis but with fewer steps. t.test(uk, conf.level=0.99) ## ## One Sample t-test ## ## data: uk ## t = 157, df = 1859, p-value &lt;2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 99 percent confidence interval: ## 3507 3624 ## sample estimates: ## mean of x ## 3566 "],
["hypothesis-tests.html", "5 Hypothesis Tests 5.1 Steps of Hypothesis Testing 5.2 Connection to Confidence Intervals 5.3 Hypothesis Testing in R", " 5 Hypothesis Tests 5.1 Steps of Hypothesis Testing Statistical hypothesis testing provides a rigorous framework for using data to provide evidence for or against claims. For example, suppose that you are working for a start-up that develops education software for children. You’re working on a new software package and are now trying to determine how much to charge. Based on experience and market trends, the leadership team thinks £50 is reasonable. As the data scientist, you are asked to do some research. The plan is for you to conduct a survey to check how much people would be willing to pay for the software. The leadership team will plan to charge £50 unless there is substantial evidence that people are willing to pay more. Your objective is to use the survey data to determine if the company should re-think the £50 price point. You design a survey and send it to \\(n=30\\) potential customers. After everyone has responded, you find that the average willingess to pay in your sample is \\(\\bar{x}=55.7\\) pounds and \\(s^2=64.8\\). n = 30 xbar = 55.7 s2 = 64.8 Now, what does this mean? We know that we cannot stop here and conclude that people are willing to pay more than £50 because if we had asked a different group of customers, our sample mean could change (and perhaps be lower than £50). Our approach will be to carry out a hypothesis test to formally decide what to do. Step 1: State the null and alternative hypotheses The framework of hypothesis testing requires us to specify two mutually exclusive hypotheses: the null hypothesis \\(H_0\\) and the alternative hypothesis \\(H_1\\). Specifically, we should choose \\(H_0\\) to be the case of “no effect” or “no change” and choose \\(H_1\\) to be the case of what we want to show. Here, we are investigating whether people are willing to pay more than £50 on average so \\(\\mu&gt;50\\) will constitute the alternative. \\[\\begin{align*} H_0:&amp;\\ \\mu\\leq50\\\\ H_1:&amp;\\ \\mu&gt;50 \\end{align*}\\] Step 2: Choose a test and significance level To determine which test is appropriate, we must first address the following questions. How many parameters do we have? (one = one-sample test, two = two-sample test) Do we know the population variance? (yes = z-test, no = t-test) In our case, we only have one parameter \\(\\mu\\) (the average WTP in the population) and we do not know the population variance, but have an estimate of it in our sample. Therefore, we should use a one-sample t-test. \\[t_{df} = {\\bar{X}-\\mu\\over S/\\sqrt{n}}\\] Finally, we will choose \\(\\alpha=0.01\\) so that we are fairly confident that if we detect deviations from £50, they reflect real deviations in the population. Step 3: Compute the observed test statistic Since we are using a one-sample t-test, our observed test statistic is: \\[t_{obs}={\\bar{x}-\\mu_0\\over s/\\sqrt{n}} = {55.7 - 50\\over \\sqrt{64.8}/\\sqrt{30}}=3.878\\] t_obs = (xbar - 50)/(sqrt(s2/n)) t_obs ## [1] 3.878 Our observed test statistic provides a measure of “evidence” against the null hypothesis. In particular, we know that under the null hypothesis, the test statistic follows a \\(t_{df}=t_{n-1}=t_{29}\\) distribution. This distribution (plotted below) represents the distribution of sample evidence given that the null is true. Our observed test statistic (the dashed red line) shows that the event we observed (\\(\\bar{x}=55.7, s^2=64.8\\)), seems fairly unlikely under the null. Our next task will be to compute this probability more formally. Step 4: Calculate the p-value The p-value is the probability of getting sample evidence as or more extreme than what we actually observed given that the null hypothesis is actually true. Remember that the test statistic is our measure of “sample evidence” – as the observed test statistic gets large, that will provide more evidence against the null hypothesis. Since we are working with a “greater-than” alternative, our p-value will be \\[\\begin{align*} \\text{p-value} &amp;= P(t_{df}&gt;t_{obs}\\ | \\ H_0 \\text{ is true})\\\\ &amp;= P\\left(t_{n-1}&gt;{\\bar{x}-\\mu_0\\over s/\\sqrt{n}}\\ \\big|\\ \\mu\\leq50\\right)\\\\ &amp;= P\\left(t_{29}&gt;{55.7 - 50\\over \\sqrt{64.8}/\\sqrt{30}}\\right)\\\\ &amp;= P\\left(t_{29}&gt;3.878\\right)\\\\ &amp;\\approx 0.0003 \\end{align*}\\] pt(t_obs,df=n-1,lower.tail=FALSE) ## [1] 0.000278 Notice that this value just corresponds to the region to the right of the observed test statistic. Since this probability is so small, it is hard to see the shaded area on the original plot. We can therefore create a “zoomed in” plot next to the original. Step 4: Make a statistical decision and interpret the results Once the p-value has been calculated, the “decision rule” can be described as follows. \\[\\begin{align*} \\text{ if p-value }\\leq \\alpha &amp;\\ \\text{ reject } H_0\\\\ \\text{ if p-value }&gt; \\alpha &amp;\\ \\text{ fail to reject } H_0\\\\ \\end{align*}\\] Where does this rule come from? Since \\(\\alpha\\) is the maximum p-value at which we reject \\(H_0\\), then we are ensuring that there is at most a \\(100\\alpha\\%\\) chance of committing a type I error. That is, if we found the p-value to be large, say 40%, then there would be a 40% chance of mistakenly deciding that the true WTP exceeded £50 when it in fact did not. For most problems, an error rate of 40% is too large to tolerate. In the social sciences, we normally choose \\(\\alpha \\in\\{0.1, 0.05, 0.01\\}\\) which corresponds to error rates of 10%, 5%, and 1%. In the context of this problem, we find the p-value is roughly 0.03%. This means that if the true average WTP in the population is less than £50, there is a 0.03% chance that we would have observed sample evidence as or more extreme than what we did observe (\\(\\bar{x}=55.7, s^2=64.8\\)). This is very small – in fact, much smaller than the 5% error rate we can tolerate. Therefore, we decide to reject the null hypothesis and conclude that it is more likely that the true average WTP in the population exceeds £50. We can take these results back to the leadership team in our company to convince them that they should consider raising the price. 5.2 Connection to Confidence Intervals There is an intimate connection between hypothesis tests and confidence intervals. We will now go through the details to see why. To start, remember that our decision rules for hypothesis testing take the following form. \\[\\begin{align*} \\text{ if p-value }\\leq \\alpha &amp;;\\ \\text{ reject } H_0\\\\ \\text{ if p-value } &gt; \\alpha &amp;;\\ \\text{ fail to reject } H_0\\\\ \\end{align*}\\] This can also be described visually. Suppose you carry out a two-sided hypothesis test with \\(\\alpha=0.05\\) and compute a test statistic \\(z_{obs}=2.054\\) and a corresponding p-value equal to 0.04. This corresponds to a total area equal to 0.04 in the lower and upper tails of the distribution of the test statistic. We can also work out what value (on the x-axis) corresponds to an area of \\(\\alpha/2=0.05/2=0.025\\) in the upper tail. qnorm(0.025,lower.tail=FALSE) ## [1] 1.96 Now the dotted black line is at the point \\(z_{\\alpha/2}=1.96\\) – i.e., the value such that the upper tail area is equal to \\(\\alpha/2=0.025\\). Notice that our shaded area falls to the right of this line, so by our decision rule, we would reject the null. But, notice that we would reject the null for any test statistic (solid red line) that falls to the right of the critical value \\(z_{\\alpha/2}\\) (dotted black line). Therefore, the following would be an equivalent set of decision rules. \\[\\begin{align*} \\text{ if } \\big|z_{obs}\\big|\\geq z_{\\alpha/2} &amp;\\ \\text{ reject } H_0\\\\ \\text{ if } \\big|z_{obs}\\big| &lt; z_{\\alpha/2} &amp;\\ \\text{ fail to reject } H_0\\\\ \\end{align*}\\] Remember that a confidence interval is a range of plausible values, which we can now formally define as the the range of parameter values that would not be rejected by our hypothesis test. In this case, the “acceptance region” is defined as follows. \\[\\begin{align*} \\big|z_{obs}\\big|&lt; z_{\\alpha/2} &amp;\\implies \\left|{\\bar{x} - \\mu_0 \\over \\sigma/\\sqrt{n}}\\right|&lt;z_{\\alpha/2}\\\\ &amp;\\implies -z_{\\alpha/2}&lt;{\\bar{x} - \\mu_0 \\over \\sigma/\\sqrt{n}}&lt;z_{\\alpha/2}\\\\ &amp;\\implies -z_{\\alpha/2} \\times {\\sigma \\over \\sqrt{n}} &lt;\\bar{x} - \\mu_0 &lt; z_{\\alpha/2} \\times {\\sigma \\over \\sqrt{n}}\\\\ &amp;\\implies -\\bar{x}-z_{\\alpha/2} \\times {\\sigma \\over \\sqrt{n}} &lt; - \\mu_0 &lt; -\\bar{x} + z_{\\alpha/2} \\times {\\sigma \\over \\sqrt{n}}\\\\ &amp;\\implies \\bar{x}-z_{\\alpha/2} \\times {\\sigma \\over \\sqrt{n}} &lt; \\mu_0 &lt; \\bar{x} + z_{\\alpha/2} \\times {\\sigma \\over \\sqrt{n}} \\end{align*}\\] This last line is the exact form of a confidence interval! 5.3 Hypothesis Testing in R We can use the t.test( ) function to carry out both one and two-sample t-tests in R. (Note: There are no built-in z-test functions in R because when we work with real data, we never know the population variance!) ONE-SAMPLE t-TEST t.test(mydata, alternative, mu, conf.level) mydata: data on the variable of interest alternative: what type of alternative hypothesis is specified? (options: “two.sided”, “greater”, “less”) mu: the value of \\(\\mu\\) under the null hypothesis conf.level: confidence level of the test (\\(1-\\alpha\\)) TWO-SAMPLE t-TEST t.test(mydata1, mydata2, alternative, mu, var.equal, conf.level) mydata1: data on the first variable of interest mydata2: data on the second variable of interest alternative: what type of alternative hypothesis is specified? (options: “two.sided”, “greater”, “less”) mu: the value of the difference \\(\\mu_1-\\mu_2\\) under the null hypothesis var.equal: do you want to assume the population variances are equal? (options: TRUE, FALSE) conf.level: confidence level of the test (\\(1-\\alpha\\)) EXAMPLE: The EuStockMarkets data set in R provides daily closing prices of four major European stock indices: Germany DAX (Ibis), Switzerland SMI, France CAC, and UK FTSE. Using this data set, test to see if there are differences in the closing prices of the SMI and CAC indices. Carry out this test at the 5% significance level and do not assume equal variances. # load the data data(EuStockMarkets) # create the SMI variable which is the second column of the EuStockMarkets data SMI = EuStockMarkets[,2] # create the CAC variable which is the third column of the EuStockMarkets data CAC = EuStockMarkets[,3] # execute the two-sample t-test t.test(SMI, CAC, alternative=&quot;two.sided&quot;, mu=0, var.equal=FALSE, conf.level=0.95) ## ## Welch Two Sample t-test ## ## data: SMI and CAC ## t = 28, df = 2305, p-value &lt;2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 1068 1228 ## sample estimates: ## mean of x mean of y ## 3376 2228 We find a p-value much smaller than \\(\\alpha=0.05\\), so we can reject the null and conclude that there are differences in the closing prices between the Swiss SMI and French CAC stock indices. "],
["regression.html", "6 Regression 6.1 Linear Regression 6.2 Regression Trees 6.3 Model Selection", " 6 Regression 6.1 Linear Regression Regression models are useful tools for (1) understanding the relationship between a response variable \\(Y\\) and a set of predictors \\(X_1,\\dotsc,X_p\\) and (2) predicting new responses \\(Y\\) from the predictors \\(X_1,\\dotsc,X_p\\). We’ll start with linear regression, which assumes that the relationship between \\(Y\\) and \\(X_1,\\dotsc,X_p\\) is linear. Let’s consider a simple example where we generate data from the following regression model. \\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\varepsilon\\] To generate data from this model, we first need to set the “true values” for the model parameters \\((\\beta_0, \\beta_1, \\beta_2)\\), generate the predictor variables \\((X_1, X_2)\\), and generate the error term \\((\\varepsilon)\\). parameters: \\(\\beta_0=-5\\), \\(\\beta_1=2\\), \\(\\beta_2=-1\\) predictor variables: \\(X_1\\sim Unif(-1,1)\\), \\(X_2\\sim Unif(-1,1)\\) error term: \\(\\varepsilon\\sim N(0,1)\\) Once we have fixed the true values of the parameters and generated predictor variables and the error term, the regression formula above tells us how to generate the response variable \\(Y\\). n = 100 beta0 = -5 beta1 = 2 beta2 = -1 X1 = runif(n,min=-1,max=1) X2 = runif(n,min=-1,max=1) epsilon = rnorm(n) Y = beta0 + beta1*X1 + beta2*X2 + epsilon Now let’s inspect the data. pairs(Y ~ X1 + X2) As we should expect, we find a positive relationship between \\(Y\\) and \\(X_1\\), a negative relationship between \\(Y\\) and \\(X_2\\), and no relationship between \\(X_1\\) and \\(X_2\\) (since they are uncorrelated). Now let’s formally estimate the model parameters \\((\\beta_0,\\beta_1,\\beta_2)\\) using R’s built-in linear model function lm( ). lm.fit = lm(Y ~ X1 + X2) summary(lm.fit) ## ## Call: ## lm(formula = Y ~ X1 + X2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.257 -0.623 0.033 0.571 2.083 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.914 0.091 -53.99 &lt; 2e-16 *** ## X1 1.666 0.159 10.48 &lt; 2e-16 *** ## X2 -1.004 0.157 -6.38 6.2e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.892 on 97 degrees of freedom ## Multiple R-squared: 0.601, Adjusted R-squared: 0.593 ## F-statistic: 73 on 2 and 97 DF, p-value: &lt;2e-16 Parameter Estimates First, focus on the “Coefficients” section. Notice that in the first column R reports estimates of our model parameters: \\(\\hat{\\beta}_0=-4.914\\), \\(\\hat{\\beta}_1=1.666\\), and \\(\\hat{\\beta}_2=-1.004\\). Since we generated this data set, we know the “true” values are \\(\\beta_0=-5\\), \\(\\beta_1=2\\), and \\(\\beta_2=-1\\). The estimates here are pretty close to the truth. (Remember: the estimates will not exactly equal the true values because we only have a random sample of \\(n=100\\) observations!) Interpretation How should we interpret the estimates? Since \\(\\hat{\\beta}_1=1.666\\), we would say that a one unit increase in \\(X_1\\) will lead to a 1.666 unit increase in \\(Y\\). Similarly, a one unit increase in \\(X_2\\) will lead to a 1.004 unit decrease in \\(Y\\). The only way to interpret the intercept is as the value of \\(Y\\) when the \\(X\\)’s are all set to zero. In many instances, setting \\(X=0\\) makes no sense, so we usually focus our attention on the coefficients attached to the predictor variables. Significance In the second, third, and fourth columns, R reports the standard error of \\(\\hat{\\beta}\\) and the t-statistic and p-value corresponding to a (one-sample) test of \\(H_0:\\beta=0\\) against \\(H_1:\\beta\\ne0\\). The asterisks next to the p-values indicate the levels (e.g., \\(\\alpha=0.05\\), \\(\\alpha=0.001\\)) for which we would conclude that the parameter is significantly different from zero. This test is naturally of interest in a regression setting because if \\(\\beta_2=0\\), for example, then \\(X_2\\) has no effect on the response \\(Y\\). Model Fit Now look at the last section where it says “Multiple R-squared: 0.601”. This value is the \\(R^2\\) statistic, which measures the percent of the variation in \\(Y\\) that is explained by the predictors. In this case, we find that 60.1% of the variation in \\(Y\\) can be explained by \\(X_1\\) and \\(X_2\\). In general, it is difficult to define an absolute scale for what a “good” \\(R^2\\) value is. In some contexts, 60% may be very high while in others it may be low. It likely depends on how difficult the response variable is to model and predict. Prediction Suppose I observed some new values of \\(X_1\\) and \\(X_2\\), say \\(X_1=0\\) and \\(X_2=0.5\\). How can I use the model to predict the corresponding value of \\(Y\\)? I could simply do the calculation by hand: \\[\\hat{Y}=\\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + \\hat{\\beta}_2X_2 =-4.914 + 1.666(0) - 1.004(0.5)=-5.416\\] where we use the “hat” notation to denote estimates or predicted values. We can also use built-in prediction tools in R (where any differences would just be due to rounding error). predict(lm.fit, newdata=data.frame(X1=0,X2=0.5)) ## 1 ## -5.415 The first argument of the predict( ) function is the regression object we created using the lm( ) function. The second argument is the new set of covariates for which we want to predict a new response \\(Y\\). (Note: the names of variables in newdata must be the same names used in the original data.) 6.2 Regression Trees A natural question to ask now is what happens if the “true” model that generated our data was not linear? For example, our model could look something like this: \\[Y_i = \\beta_0 + {\\beta_1X_{1i} \\over\\beta_2 + X_{2i}} + \\varepsilon_i\\] Here we still have three model parameters (\\(\\beta_0,\\beta_1,\\beta_2\\)), but they enter the regression function in a nonlinear fashion. If we generate data from this model and then estimate the linear regression model from section 1, what will happen? # generate data n = 100 beta0 = -5 beta1 = 2 beta2 = -1 X1 = runif(n,min=-1,max=1) X2 = runif(n,min=-1,max=1) epsilon = rnorm(n) Y = beta0 + beta1*X1/(beta2+X2) + epsilon # estimate linear regression model lm.fit = lm(Y ~ X1 + X2) summary(lm.fit) ## ## Call: ## lm(formula = Y ~ X1 + X2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.93 -8.55 -1.96 4.48 165.50 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.81 2.15 -1.77 0.07952 . ## X1 -14.36 3.60 -3.99 0.00013 *** ## X2 5.73 3.51 1.63 0.10560 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 21.2 on 97 degrees of freedom ## Multiple R-squared: 0.17, Adjusted R-squared: 0.153 ## F-statistic: 9.91 on 2 and 97 DF, p-value: 0.000121 The answer is that we get incorrect estimates of model parameters! (Remember \\(\\beta=-5,\\beta_1=2,\\beta_2=-1\\).) A more flexible approach to regression modeling is provided by regression trees. The idea is to split up the covariate space into homogeneous regions (with respect to the response \\(Y\\)) and then fit simple linear models within each region. We can use the rpart library in R to fit and plot regression trees. You’ll actually notice a similar syntax between lm( ) and rpart( ). library(rpart) # estimate regression tree tree.fit = rpart(Y ~ X1 + X2) # plot the estimated tree plot(tree.fit, uniform=TRUE, margin=.05) text(tree.fit) The output from a regression tree model looks very different from the output of a linear regression model. This is mostly because we had real-valued parameters in the linear model, but have much more complicated parameters in the tree model. The top node is called the root node and indicates the most important variable for predicting \\(Y\\). Each subsequent node is called an interior node until you get to the last node showing a numeric value which is called a terminal node. Tree models should be interpreted as a sequence of decisions for the purposes of making a prediction. Each node will present a logical statement and if that statement is true, we move down and to the left whereas if that statement is false, we move down and to the right. For example, if you wanted to predict \\(Y\\) when \\(X_1=0\\) and \\(X_2=0.5\\), the root note first asks “Is \\(X_1\\geq -0.9113\\)?” If yes, then left and if no then right. Here our answer is yes, so we go to the next node to the left and ask “Is \\(X_1\\geq0.264\\)?” Our answer is no so we go to the right and ask \\(X_2&lt;0.6835\\)?&quot; Our answer is yes so we go to the left. Since this represents the terminal node, we’re left with our predition of \\(\\hat{Y}=-4.037\\). That is, if \\(X_1=1\\) and \\(X_2=9\\) then the model predicts \\(\\hat{Y}=-4.037\\). We can also use the predict( ) function as we did with the linear regression model above. predict(tree.fit, newdata=data.frame(X1=0,X2=0.5)) ## 1 ## -4.037 6.3 Model Selection Let’s see how regression trees compare to linear regression models in terms of out-of-sample prediction. We’ll consider two cases: The true model is a linear model The true model is a nonlinear model CASE A: TRUE MODEL IS LINEAR First, we’ll generate a training and test data set from a linear regression model as in section 1. The training data set will be used for estimation and the test data will be used for prediction. n = 100 beta0 = -5 beta1 = 2 beta2 = -1 X1 = runif(n,min=-1,max=1) X2 = runif(n,min=-1,max=1) epsilon = rnorm(n) Y = beta0 + beta1*X1 + beta2*X2 + epsilon train = data.frame(Y=Y[1:70], X1=X1[1:70], X2=X2[1:70]) test = data.frame(Y=Y[71:100], X1=X1[71:100], X2=X2[71:100]) Now let’s estimate both the linear regression and regression tree models on the training data. # estimate linear regression model lm.fit = lm(Y ~ X1 + X2, data=train) # estimate regression tree model tree.fit = rpart(Y ~ X1 + X2, data=train) To compare out-of-sample model performance, we’ll compute the root mean squared error (RMSE). \\[\\text{RMSE}=\\sqrt{{1\\over n}\\sum_{i=1}^n (\\hat{y}_i-y_i)^2}\\] # linear regression model lm.predict = predict(lm.fit, newdata=test) lm.rmse = sqrt(mean((lm.predict-test$Y)^2)) lm.rmse ## [1] 0.9157 # regression tree model tree.predict = predict(tree.fit, newdata=test) tree.rmse = sqrt(mean((tree.predict - test$Y)^2)) tree.rmse ## [1] 1.149 In this case the linear regression model has better predictive performance, which is not too surprising because we simulated the data from that model! CASE B: TRUE MODEL IS NONLINEAR We will again generate a training and test data set, but now from the nonlinear regression model we used in section 2. n = 100 beta0 = -5 beta1 = 2 beta2 = -1 X1 = runif(n,min=-1,max=1) X2 = runif(n,min=-1,max=1) epsilon = rnorm(n) Y = beta0 + beta1*X1/(beta2+X2) + epsilon train = data.frame(Y=Y[1:70], X1=X1[1:70], X2=X2[1:70]) test = data.frame(Y=Y[71:100], X1=X1[71:100], X2=X2[71:100]) Let’s again estimate both the linear regression and regression tree models on the training data and compute the predictive RMSE. # linear regression model lm.fit = lm(Y ~ X1 + X2, data=train) lm.predict = predict(lm.fit, newdata=test) lm.rmse = sqrt(mean((lm.predict - test$Y)^2)) lm.rmse ## [1] 18.76 # regression tree model tree.fit = rpart(Y ~ X1 + X2, data=train) tree.predict = predict(tree.fit, newdata=test) tree.rmse = sqrt(mean((tree.predict - test$Y)^2)) tree.rmse ## [1] 17.35 Now the regression tree model has better predictive performance (but notice that the linear model still does relatively well!) In general, regression trees suffer from a problem called overfitting: the trees learn too much from the training data that they don’t generalize well to test data. There are ways of correcting for this, and you will learn more about them in Data Analyics II! "],
["classification.html", "7 Classification 7.1 \\(k\\)-Nearest Neighbors 7.2 Logistic Regression 7.3 Classification Trees", " 7 Classification Classification shares many similarities with regression: We have a response variable \\(Y\\) and a set of one or more predictors \\(X_1,\\dotsc,X_p\\). The difference is that for classification problems, the response \\(Y\\) is discrete, meaning \\(Y\\in\\{1,2,\\dotsc,C\\}\\) where \\(C\\) is the number of classes that \\(Y\\) can take on. We will focus our attention on binary responses \\(Y\\in\\{0,1\\}\\), but all of the methods we discuss can be extended to the more general case outlined above. To illustrate classification methods, we will use the Default data in the ISLR R library. The data set contains four variables: default is an indicator of whether the customer defaulted on their debt, student is an indicator of whether the customer is a student, balance is the average balance that the customer has remaining on their credit card after making their monthly payment, and income is the customer’s income. library(ISLR) data(Default) head(Default) ## default student balance income ## 1 No No 729.5 44362 ## 2 No Yes 817.2 12106 ## 3 No No 1073.5 31767 ## 4 No No 529.3 35704 ## 5 No No 785.7 38463 ## 6 No Yes 919.6 7492 We also need to split up the data into training and test samples in order to measure the predictive accuracy of different approaches to classification. train = Default[1:7000,] test = Default[7001:10000,] 7.1 \\(k\\)-Nearest Neighbors The \\(k\\)-NN algorithms are built on the following idea: given a new observation \\(X^*\\) for which we want to predict an associated response \\(Y^*\\), we can find values of \\(X\\) in our data that look similar to \\(X^*\\) and then classify \\(Y^*\\) based on the associated \\(Y\\)’s. We will use Euclidean distance is a measure of similarity (which is only defined for real-valued \\(X\\)’s). Let’s take a small portion (first 10 rows) of the Default data to work through a simple example. Notice that we will exclude the student variable since it is a categorical rather than numeric variable. We will use the 11th observation as our “test” data \\(X^*\\) that we want to make predictions for. X = Default[1:10,3:4] Y = Default[1:10,1] newX = Default[11,3:4] We now need to compute the similarity (i.e., Euclidean distance) between \\(X^*=(X_1^*,X_2^*)\\) and \\(X_i=(X_{1i},X_{2i})\\) for each \\(i=1,\\dotsc,n\\). \\[dist(X^*,X_i)=||X^*-X_i||=\\sqrt{(X_1^*-X_{1i})^2+(X_2^*-X_{2i})^2}\\] To do this in R, we can take use the apply( ) function. The first argument is the matrix of \\(X\\) variables that we want to cycle through to compare with \\(X^*\\). The second argument of the apply( ) function tells R whether we want to perform an operation for each row (=1) of for each column (=2). The last row tells R what function we want to compute. Here, we need to evaluate \\(dist(X^*,X_i)\\) for each row. distance = apply(X,1,function(x)sqrt(sum((x-newX)^2))) distance ## 1 2 3 4 5 6 7 8 9 10 ## 22502 9799 9954 13844 16611 14409 3144 4347 15641 7404 Notice that the function returns a set of 10 distances. If we wanted to use the 1st-nearest neighbor classifier to predict \\(Y^*\\), for example, then we would need to find the \\(Y\\) value of \\(X_i\\) for the observation \\(i\\) that has the smallest distance. We can find that value using the which.min( ) function. which.min(distance) ## 7 ## 7 Y[which.min(distance)] ## [1] No ## Levels: No Yes Therefore, we would predict \\(Y^*=No\\) having observed \\(X^*\\). Now let’s go back to the full data set and test the performance of the \\(k\\)-NN classifier. The first thing we should do is standardize the \\(X\\)’s since the nearest neighbors algorithm depends on the scale of the covariates. stdtrainX = scale(train[,3:4]) stdtestX = scale(test[,3:4]) summary(stdtrainX) ## balance income ## Min. :-1.728 Min. :-2.4632 ## 1st Qu.:-0.733 1st Qu.:-0.9207 ## Median :-0.030 Median : 0.0804 ## Mean : 0.000 Mean : 0.0000 ## 3rd Qu.: 0.686 3rd Qu.: 0.7730 ## Max. : 3.454 Max. : 3.0059 Now we can use the knn( ) function in the class R library to run the algorithm on the training data and then make predictions for each observation in the test data. The first argument calls for the \\(X\\)’s in the training data, the second calls for the \\(X\\)’s in the test data (for which we want to predict), the third calls for the \\(Y\\)’s in the training data, and the fourth calls for \\(k\\), the number of nearest neighbors we want to use to make the prediction. library(class) knn1 = knn(stdtrainX, stdtestX, train$default, k=1) The knn1 object now contains a vector of predicted \\(Y\\)’s for each value of \\(X\\) in the test data. We can then compare the predicted response \\(\\hat{Y}\\) to the true response in the test data \\(Y\\) to assess the performance of the classification algorithm. In particular, we will see the fraction of predictions the algorithm gets wrong. mean(knn1 != test$default) ## [1] 0.04467 In this case, the 1-NN classifier as an error rate of about 4.5% (or equivalently, an accuracy of 95.5%). We can try increasing \\(k\\) to see if there is any effect on predictive fit. # 5 nearest neighbors knn5 = knn(stdtrainX, stdtestX, train$default, k=5) mean(knn5 != test$default) ## [1] 0.029 # 10 nearest neighbors knn10 = knn(stdtrainX, stdtestX, train$default, k=10) mean(knn10 != test$default) ## [1] 0.026 # 50 nearest neighbors knn50 = knn(stdtrainX, stdtestX, train$default, k=50) mean(knn50 != test$default) ## [1] 0.024 # 100 nearest neighbors knn100 = knn(stdtrainX, stdtestX, train$default, k=100) mean(knn100 != test$default) ## [1] 0.027 We would then likely choose the model that predicts best (i.e., has the lowest error/misclassification rate). The last object of interest when doing classification is the confusion matrix, which allows us to decompose misclassification mistakes into two groups: false positives (predict \\(\\hat{Y}=1\\) when \\(Y=0\\)) and false negatives (predict \\(\\hat{Y}=0\\) when \\(Y=1\\)). Let’s produce the confusion matrix for the 10-NN classifier. table(knn10,test$default) ## ## knn10 No Yes ## No 2889 60 ## Yes 18 33 # false positive rate 18/(18+2889) ## [1] 0.006192 # false negative rate 60/(60+33) ## [1] 0.6452 The false negative rate is especially high, which would be concerning given the risks to the lending agency (e.g., bank). 7.2 Logistic Regression Issues with the \\(k\\)-NN algorithms include the fact they can’t accommodate categorical \\(X\\)’s, the algorithms aren’t based on a formal statistical model so we can’t do inference (or learn about how the \\(X\\)’s relate to \\(Y\\)), and there is an assumption that all \\(X\\)’s matter and matter equally in determining \\(Y\\). Our first solution to these problems is logistic regression. Given a response \\(Y\\in\\{0,1\\}\\) and a set of predictors \\(X_1,\\dotsc,X_P\\), the logistic regression model is written as follows. \\[\\text{Pr}(Y=1|X)={\\exp(\\beta_0+\\beta_1X_1+\\dotsc+\\beta_pX_p)\\over 1 + \\exp(\\beta_0+\\beta_1X_1+\\dotsc+\\beta_pX_p)}\\] The intuition for this formula is as follows. If \\(Y\\in\\{0,1\\}\\), then we can assume that \\(Y\\sim\\text{Bernoulli}(\\theta)\\) where \\(\\theta=\\text{Pr}(Y=1)\\). We can then write down a regression model for \\(\\theta\\) rather than \\(Y\\). The only remaining problem is that \\(\\theta\\in(0,1)\\), so we need to transform the linear regression function \\(h(X)=\\beta_0+\\beta_1X_1+\\dotsc+\\beta_pX_p)\\) in a way so that it is constrained to be between 0 and 1. The function \\(e^{h(X)}/(1 + e^{h(X)})\\) does just that. Estimating a logistic regression model in R can be done using the glm( ) function, which is similar to the lm( ) command we use to estimate linear regression models. Let’s illustrate with the training sample from the Default data set. glm.fit = glm(default ~ student + balance + income, family=&quot;binomial&quot;, data=train) summary(glm.fit) ## ## Call: ## glm(formula = default ~ student + balance + income, family = &quot;binomial&quot;, ## data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.187 -0.142 -0.055 -0.020 3.686 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.10e+01 5.89e-01 -18.70 &lt;2e-16 *** ## studentYes -6.46e-01 2.85e-01 -2.27 0.023 * ## balance 5.83e-03 2.78e-04 20.96 &lt;2e-16 *** ## income 4.71e-06 9.87e-06 0.48 0.633 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2090.7 on 6999 degrees of freedom ## Residual deviance: 1109.4 on 6996 degrees of freedom ## AIC: 1117 ## ## Number of Fisher Scoring iterations: 8 Notice that we added one more option in the glm( ) function: type=&quot;binomial&quot;. This option tells R to use the logistic regression model rather than other types of generalized linear models. The output from the logistic regression model looks fairly similar to that of linear regression models. However, the interpretation of model paramters (and their estimates) changes a bit. For example, we find that the coefficient on balance is estimated to be about 0.0058, which means that a one dollar increase in balance multiplies the odds of default by exp(0.0058)=1.006. Since this number is greater than 1, we can say that increasing the balance increases the odds of default. To predict responses in the test data, we can use the predict( ) function in R. We again need to add one option: type=&quot;response&quot;, which will tell R to return the predicted probabilities that \\(Y=1\\). glm.probs = predict(glm.fit, newdata=test, type=&quot;response&quot;) Then we can compute \\(\\hat{Y}\\) by using the rule that \\(\\hat{Y}=\\text{Yes}\\) if the predicted probability is greater than 0.5 and \\(\\hat{Y}=\\text{No}\\) otherwise. glm.predict = ifelse(glm.probs&gt;0.5,&quot;Yes&quot;,&quot;No&quot;) Just as before, we can compare the model predictions with the actual \\(Y\\)’s in the test data to compute the out-of-sample error (misclassification) rate. mean(glm.predict != test$default) ## [1] 0.024 This error rate can be decomposed by producing the associated confusion matrix and computing the false positive and false negative rates. table(glm.predict, test$default) ## ## glm.predict No Yes ## No 2896 61 ## Yes 11 32 # false positive rate 11/(11+2896) ## [1] 0.003784 # false negative rate 61/(61+32) ## [1] 0.6559 7.3 Classification Trees Classification trees offer the same advantages over logistic regression that regression trees do for linear regression. That is, classification trees provide a classification rule that does not assume any form of linearity in the covariates \\(X\\). The nice thing is their implimentation in R is nearly identical to that of regression trees. library(rpart) # estimate regression tree tree.fit = rpart(default ~ student + balance + income, method=&quot;class&quot;, data=train) # plot estimated tree plot(tree.fit,uniform=TRUE,margin=0.05,main=&quot;DEFAULT&quot;) text(tree.fit) We can again use the predict( ) function to predict the response values for the test data and compute the out-of-sample error (misclassification) rate. We need to specify the type=&quot;class&quot; option so that the predict( ) function returns the predicted values \\(\\hat{Y}\\). tree.predict = predict(tree.fit, newdata=test, type=&quot;class&quot;) mean(tree.predict != test$default) ## [1] 0.027 Finally, the error rate can be decomposed by producing the associated confusion matrix and computing the false positive and false negative rates. table(tree.predict, test$default) ## ## tree.predict No Yes ## No 2880 54 ## Yes 27 39 # false positive rate 27/(27+2880) ## [1] 0.009288 # false negative rate 54/(54+39) ## [1] 0.5806 "],
["clustering.html", "8 Clustering 8.1 \\(k\\)-Means 8.2 Hierarchical Clustering", " 8 Clustering The goal of clustering is to discover groups of similar observations among a set of variables \\(X_1,\\dotsc,X_p\\). Clustering is an example of an unsupervised learning method, as we only consider the features \\(X_1,\\dotsc,X_p\\) without an associated response \\(Y\\). To illustrate clustering methods, we will use the Auto data in the ISLR R library. The data set contains information on the gas mileage, number of cylinders, displacement, horsepower, weight, acceleration, year, and origin for 392 vehicles. library(ISLR) # load data data(Auto) attach(Auto) # inspect first few rows head(Auto) mpg cylinders displacement horsepower weight acceleration year origin name 18 8 307 130 3504 12.0 70 1 chevrolet chevelle malibu 15 8 350 165 3693 11.5 70 1 buick skylark 320 18 8 318 150 3436 11.0 70 1 plymouth satellite 16 8 304 150 3433 12.0 70 1 amc rebel sst 17 8 302 140 3449 10.5 70 1 ford torino 15 8 429 198 4341 10.0 70 1 ford galaxie 500 8.1 \\(k\\)-Means The \\(k\\)-means clustering algorithm uses the variables \\(X_1,\\dotsc,X_p\\) to partition our observations \\(1,\\dotsc,n\\) into \\(k\\) non-overlapping groups. The partitioning is done based on the similarity of observations, where similarity is measured using Euclidean distance. Consequently, we will need to rescale our data. We’ll isolate the first seven variables (mpg, cylinders, displacement, horsepower, weight, acceleration, year) and define them as \\(X\\). # select first seven columns of Auto data X = Auto[,1:7] # rescale X&#39;s stdX = scale(X) # set the name of each row to be the car name stored in the Auto data rownames(stdX) = Auto$name # summarize the rescaled data summary(stdX) ## mpg cylinders displacement horsepower ## Min. :-1.8509 Min. :-1.449 Min. :-1.208 Min. :-1.519 ## 1st Qu.:-0.8259 1st Qu.:-0.863 1st Qu.:-0.854 1st Qu.:-0.766 ## Median :-0.0892 Median :-0.863 Median :-0.415 Median :-0.285 ## Mean : 0.0000 Mean : 0.000 Mean : 0.000 Mean : 0.000 ## 3rd Qu.: 0.7116 3rd Qu.: 1.482 3rd Qu.: 0.777 3rd Qu.: 0.559 ## Max. : 2.9666 Max. : 1.482 Max. : 2.490 Max. : 3.261 ## weight acceleration year ## Min. :-1.607 Min. :-2.733 Min. :-1.6232 ## 1st Qu.:-0.886 1st Qu.:-0.640 1st Qu.:-0.8089 ## Median :-0.205 Median :-0.015 Median : 0.0055 ## Mean : 0.000 Mean : 0.000 Mean : 0.0000 ## 3rd Qu.: 0.750 3rd Qu.: 0.538 3rd Qu.: 0.8199 ## Max. : 2.546 Max. : 3.356 Max. : 1.6343 Let’s start by runing the \\(k\\)-means algorithm with \\(k=2\\) and only using the mpg and horsepower variables. # estimate clusters km2 = kmeans(stdX[,c(1,4)],2) # plot clusters plot(mpg, horsepower, col=km2$cluster) We can see how the algorithm divides the observations into two groups: the red observations have a high mpg but lower horsepower, while the black observations have a low mpg but high horsepower. Now let’s try using all seven variables to define the clusters. # estimate clusters km2 = kmeans(stdX,2) # plot clusters pver mpg and horsepower plot(mpg, horsepower, col=km2$cluster) The plot looks similar: even when we use all variables, the first group of cars (black observations) have a low mpg and high horsepower while the second group (red observations) have a high mpg and low horsepower. The plot above only shows the clustering solution with respect to two variables (mpg and horsepower). To examine how the clusters are defined over all variables, we can use the pairs( ) function. # plot clusters over all variables pairs(stdX, col=km2$cluster, xaxt=&quot;n&quot;, yaxt=&quot;n&quot;) Lastly, we can explore clustering solutions for different values of \\(k\\). For simplicity, we will only examine the clusters for the mpg and horsepower variables. # estimate clusters km3 = kmeans(stdX,3) km4 = kmeans(stdX,4) km5 = kmeans(stdX,5) # plot clusters over mpg and horsepower par(mfrow=c(2,2), mar=c(4.1,4.1,2.1,2.1)) plot(mpg, horsepower, col=km2$cluster) plot(mpg, horsepower, col=km3$cluster) plot(mpg, horsepower, col=km4$cluster) plot(mpg, horsepower, col=km5$cluster) As \\(k\\) increases, we get a more granular picture of car segments. However, the problem of interpretation becomes more difficult: What is it that actually differentiates these clusters from each other? We are only plotting the data over two variables, but the other variables also contribute in the determination of cluster assignments. Moreover, how should we determine an appropriate value of \\(k\\)? Hierachical clustering provides a partial solution. 8.2 Hierarchical Clustering Hierarchical clustering addresses the issue of having to choose the number of clusters \\(k\\), and instead considers a sequence of clusters from \\(k = 1,\\dotsc,n\\). We’ll use the hclust( ) function and dendextend package to fit and plot the output from hierarchical clustering models. library(dendextend) # estimate clusters hc = hclust(dist(X)) # plot clusters dend = as.dendrogram(hc) labels_cex(dend) = .25 plot(dend) Because we have a few hundred observations, the plot – which called a “dendrogram” – becomes difficult to read and interpret on a small scale (meaning we would need a much larger plotting window!). Suppose we were interested in a two group clustering solution. Then we can simply color the dendrogram based on the first split. dend = as.dendrogram(hc) dend = color_labels(dend,k=2) dend = color_branches(dend,k=2) labels_cex(dend) = .25 plot(dend) We can do the same for a larger number of groups, too. dend = as.dendrogram(hc) dend = color_labels(dend,k=10) dend = color_branches(dend,k=10) labels_cex(dend) = .25 plot(dend) Notice that when interpreting a dendrogram, the analyst must still “choose” \\(k\\), so the problem still hasn’t really gone away. The only benefit with hierarchical clustering methods is that the analyst can quickly and easily examine the landscape of clustering solutions to understand how the value of \\(k\\) impacts different clustering solutions. "]
]
