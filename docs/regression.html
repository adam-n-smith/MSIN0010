<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Regression | Data Analytics with R</title>
  <meta name="description" content="Chapter 6 Regression | Data Analytics with R" />
  <meta name="generator" content="bookdown 0.14 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Regression | Data Analytics with R" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Regression | Data Analytics with R" />
  
  
  

<meta name="author" content="Adam Smith, UCL School of Management" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="hypothesis-tests.html"/>
<link rel="next" href="classification.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html"><i class="fa fa-check"></i>Getting Started with R</a><ul>
<li class="chapter" data-level="" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#why-use-r"><i class="fa fa-check"></i>Why use R?</a></li>
<li class="chapter" data-level="" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#downloading-r-and-rstudio"><i class="fa fa-check"></i>Downloading R and RStudio</a></li>
<li class="chapter" data-level="" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#installing-and-loading-packages"><i class="fa fa-check"></i>Installing and Loading Packages</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction-to-data-analytics.html"><a href="introduction-to-data-analytics.html"><i class="fa fa-check"></i><b>1</b> Introduction to Data Analytics</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction-to-data-analytics.html"><a href="introduction-to-data-analytics.html#loading-and-inspecting-data-sets"><i class="fa fa-check"></i><b>1.1</b> Loading and Inspecting Data Sets</a></li>
<li class="chapter" data-level="1.2" data-path="introduction-to-data-analytics.html"><a href="introduction-to-data-analytics.html#statistical-summaries"><i class="fa fa-check"></i><b>1.2</b> Statistical Summaries</a></li>
<li class="chapter" data-level="1.3" data-path="introduction-to-data-analytics.html"><a href="introduction-to-data-analytics.html#graphical-summaries"><i class="fa fa-check"></i><b>1.3</b> Graphical Summaries</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="computing-probabilities.html"><a href="computing-probabilities.html"><i class="fa fa-check"></i><b>2</b> Computing Probabilities</a><ul>
<li class="chapter" data-level="2.1" data-path="computing-probabilities.html"><a href="computing-probabilities.html#normal-distribution"><i class="fa fa-check"></i><b>2.1</b> Normal Distribution</a><ul>
<li class="chapter" data-level="2.1.1" data-path="computing-probabilities.html"><a href="computing-probabilities.html#probability-density-function-dnorm"><i class="fa fa-check"></i><b>2.1.1</b> Probability Density Function (<code>dnorm</code>)</a></li>
<li class="chapter" data-level="2.1.2" data-path="computing-probabilities.html"><a href="computing-probabilities.html#cumulative-distribution-function-pnorm"><i class="fa fa-check"></i><b>2.1.2</b> Cumulative Distribution Function (<code>pnorm</code>)</a></li>
<li class="chapter" data-level="2.1.3" data-path="computing-probabilities.html"><a href="computing-probabilities.html#quantile-function-qnorm"><i class="fa fa-check"></i><b>2.1.3</b> Quantile Function (<code>qnorm</code>)</a></li>
<li class="chapter" data-level="2.1.4" data-path="computing-probabilities.html"><a href="computing-probabilities.html#random-number-generator-rnorm"><i class="fa fa-check"></i><b>2.1.4</b> Random Number Generator (<code>rnorm</code>)</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="computing-probabilities.html"><a href="computing-probabilities.html#bernoulli-and-binomial-distributions"><i class="fa fa-check"></i><b>2.2</b> Bernoulli and Binomial Distributions</a><ul>
<li class="chapter" data-level="2.2.1" data-path="computing-probabilities.html"><a href="computing-probabilities.html#probability-mass-function-dbinom"><i class="fa fa-check"></i><b>2.2.1</b> Probability Mass Function (<code>dbinom</code>)</a></li>
<li class="chapter" data-level="2.2.2" data-path="computing-probabilities.html"><a href="computing-probabilities.html#cumulative-distribution-function-pbinom"><i class="fa fa-check"></i><b>2.2.2</b> Cumulative Distribution Function (<code>pbinom</code>)</a></li>
<li class="chapter" data-level="2.2.3" data-path="computing-probabilities.html"><a href="computing-probabilities.html#quantile-function-qbinom"><i class="fa fa-check"></i><b>2.2.3</b> Quantile Function (<code>qbinom</code>)</a></li>
<li class="chapter" data-level="2.2.4" data-path="computing-probabilities.html"><a href="computing-probabilities.html#random-number-generator-rbinom"><i class="fa fa-check"></i><b>2.2.4</b> Random Number Generator (<code>rbinom</code>)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="sampling-distributions-and-the-clt.html"><a href="sampling-distributions-and-the-clt.html"><i class="fa fa-check"></i><b>3</b> Sampling Distributions and the CLT</a></li>
<li class="chapter" data-level="4" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>4</b> Estimation</a><ul>
<li class="chapter" data-level="4.1" data-path="estimation.html"><a href="estimation.html#point-estimation-and-the-mle"><i class="fa fa-check"></i><b>4.1</b> Point Estimation and the MLE</a></li>
<li class="chapter" data-level="4.2" data-path="estimation.html"><a href="estimation.html#confidence-intervals"><i class="fa fa-check"></i><b>4.2</b> Confidence Intervals</a><ul>
<li class="chapter" data-level="4.2.1" data-path="estimation.html"><a href="estimation.html#types-of-confidence-intervals"><i class="fa fa-check"></i><b>4.2.1</b> Types of Confidence Intervals</a></li>
<li class="chapter" data-level="4.2.2" data-path="estimation.html"><a href="estimation.html#examples"><i class="fa fa-check"></i><b>4.2.2</b> Examples</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hypothesis-tests.html"><a href="hypothesis-tests.html"><i class="fa fa-check"></i><b>5</b> Hypothesis Tests</a><ul>
<li class="chapter" data-level="5.1" data-path="hypothesis-tests.html"><a href="hypothesis-tests.html#steps-of-hypothesis-testing"><i class="fa fa-check"></i><b>5.1</b> Steps of Hypothesis Testing</a></li>
<li class="chapter" data-level="5.2" data-path="hypothesis-tests.html"><a href="hypothesis-tests.html#connection-to-confidence-intervals"><i class="fa fa-check"></i><b>5.2</b> Connection to Confidence Intervals</a></li>
<li class="chapter" data-level="5.3" data-path="hypothesis-tests.html"><a href="hypothesis-tests.html#hypothesis-testing-in-r"><i class="fa fa-check"></i><b>5.3</b> Hypothesis Testing in R</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>6</b> Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="regression.html"><a href="regression.html#linear-regression"><i class="fa fa-check"></i><b>6.1</b> Linear Regression</a></li>
<li class="chapter" data-level="6.2" data-path="regression.html"><a href="regression.html#regression-trees"><i class="fa fa-check"></i><b>6.2</b> Regression Trees</a></li>
<li class="chapter" data-level="6.3" data-path="regression.html"><a href="regression.html#model-selection"><i class="fa fa-check"></i><b>6.3</b> Model Selection</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>7</b> Classification</a><ul>
<li class="chapter" data-level="7.1" data-path="classification.html"><a href="classification.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>7.1</b> <span class="math inline">\(k\)</span>-Nearest Neighbors</a></li>
<li class="chapter" data-level="7.2" data-path="classification.html"><a href="classification.html#logistic-regression"><i class="fa fa-check"></i><b>7.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="7.3" data-path="classification.html"><a href="classification.html#classification-trees"><i class="fa fa-check"></i><b>7.3</b> Classification Trees</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>8</b> Clustering</a><ul>
<li class="chapter" data-level="8.1" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>8.1</b> <span class="math inline">\(k\)</span>-Means</a></li>
<li class="chapter" data-level="8.2" data-path="clustering.html"><a href="clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>8.2</b> Hierarchical Clustering</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analytics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Regression</h1>
<div id="linear-regression" class="section level2">
<h2><span class="header-section-number">6.1</span> Linear Regression</h2>
<hr>
<p>Regression models are useful tools for (1) understanding the relationship between a response variable <span class="math inline">\(Y\)</span> and a set of predictors <span class="math inline">\(X_1,\dotsc,X_p\)</span> and (2) predicting new responses <span class="math inline">\(Y\)</span> from the predictors <span class="math inline">\(X_1,\dotsc,X_p\)</span>.</p>
<p>We’ll start with <strong>linear regression</strong>, which assumes that the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1,\dotsc,X_p\)</span> is linear.</p>
<p>Let’s consider a simple example where we generate data from the following regression model.</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \varepsilon\]</span></p>
<p>To generate data from this model, we first need to set the “true values” for the model parameters <span class="math inline">\((\beta_0, \beta_1, \beta_2)\)</span>, generate the predictor variables <span class="math inline">\((X_1, X_2)\)</span>, and generate the error term <span class="math inline">\((\varepsilon)\)</span>.</p>
<ul>
<li>parameters: <span class="math inline">\(\beta_0=-5\)</span>, <span class="math inline">\(\beta_1=2\)</span>, <span class="math inline">\(\beta_2=-1\)</span></li>
<li>predictor variables: <span class="math inline">\(X_1\sim Unif(-1,1)\)</span>, <span class="math inline">\(X_2\sim Unif(-1,1)\)</span></li>
<li>error term: <span class="math inline">\(\varepsilon\sim N(0,1)\)</span></li>
</ul>
<p>Once we have fixed the true values of the parameters and generated predictor variables and the error term, the regression formula above tells us how to generate the response variable <span class="math inline">\(Y\)</span>.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb144-1" data-line-number="1">n =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb144-2" data-line-number="2">beta0 =<span class="st"> </span><span class="dv">-5</span></a>
<a class="sourceLine" id="cb144-3" data-line-number="3">beta1 =<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb144-4" data-line-number="4">beta2 =<span class="st"> </span><span class="dv">-1</span></a>
<a class="sourceLine" id="cb144-5" data-line-number="5">X1 =<span class="st"> </span><span class="kw">runif</span>(n,<span class="dt">min=</span><span class="op">-</span><span class="dv">1</span>,<span class="dt">max=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb144-6" data-line-number="6">X2 =<span class="st"> </span><span class="kw">runif</span>(n,<span class="dt">min=</span><span class="op">-</span><span class="dv">1</span>,<span class="dt">max=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb144-7" data-line-number="7">epsilon =<span class="st"> </span><span class="kw">rnorm</span>(n)</a>
<a class="sourceLine" id="cb144-8" data-line-number="8">Y =<span class="st"> </span>beta0 <span class="op">+</span><span class="st"> </span>beta1<span class="op">*</span>X1 <span class="op">+</span><span class="st"> </span>beta2<span class="op">*</span>X2 <span class="op">+</span><span class="st"> </span>epsilon</a></code></pre></div>
<p>Now let’s inspect the data.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb145-1" data-line-number="1"><span class="kw">pairs</span>(Y <span class="op">~</span><span class="st"> </span>X1 <span class="op">+</span><span class="st"> </span>X2)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-88-1.png" width="672" /></p>
<p>As we should expect, we find a positive relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1\)</span>, a negative relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_2\)</span>, and no relationship between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> (since they are uncorrelated).</p>
<p>Now let’s formally estimate the model parameters <span class="math inline">\((\beta_0,\beta_1,\beta_2)\)</span> using R’s built-in linear model function <code>lm( )</code>.</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb146-1" data-line-number="1">lm.fit =<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X1 <span class="op">+</span><span class="st"> </span>X2)</a>
<a class="sourceLine" id="cb146-2" data-line-number="2"><span class="kw">summary</span>(lm.fit)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X1 + X2)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -2.257 -0.623  0.033  0.571  2.083 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   -4.914      0.091  -53.99  &lt; 2e-16 ***
## X1             1.666      0.159   10.48  &lt; 2e-16 ***
## X2            -1.004      0.157   -6.38  6.2e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.892 on 97 degrees of freedom
## Multiple R-squared:  0.601,  Adjusted R-squared:  0.593 
## F-statistic:   73 on 2 and 97 DF,  p-value: &lt;2e-16</code></pre>
<p><font size="4"><strong>Parameter Estimates</strong></font></p>
<p>First, focus on the “Coefficients” section. Notice that in the first column R reports estimates of our model parameters: <span class="math inline">\(\hat{\beta}_0=-4.914\)</span>, <span class="math inline">\(\hat{\beta}_1=1.666\)</span>, and <span class="math inline">\(\hat{\beta}_2=-1.004\)</span>. Since we generated this data set, we know the “true” values are <span class="math inline">\(\beta_0=-5\)</span>, <span class="math inline">\(\beta_1=2\)</span>, and <span class="math inline">\(\beta_2=-1\)</span>. The estimates here are pretty close to the truth. (Remember: the estimates will not exactly equal the true values because we only have a random sample of <span class="math inline">\(n=100\)</span> observations!)</p>
<p><font size="4"><strong>Interpretation</strong></font></p>
<p>How should we interpret the estimates? Since <span class="math inline">\(\hat{\beta}_1=1.666\)</span>, we would say that a one unit increase in <span class="math inline">\(X_1\)</span> will lead to a 1.666 unit <em>increase</em> in <span class="math inline">\(Y\)</span>. Similarly, a one unit increase in <span class="math inline">\(X_2\)</span> will lead to a 1.004 unit <em>decrease</em> in <span class="math inline">\(Y\)</span>. The only way to interpret the intercept is as the value of <span class="math inline">\(Y\)</span> when the <span class="math inline">\(X\)</span>’s are all set to zero. In many instances, setting <span class="math inline">\(X=0\)</span> makes no sense, so we usually focus our attention on the coefficients attached to the predictor variables.</p>
<p><font size="4"><strong>Significance</strong></font></p>
<p>In the second, third, and fourth columns, R reports the standard error of <span class="math inline">\(\hat{\beta}\)</span> and the t-statistic and p-value corresponding to a (one-sample) test of <span class="math inline">\(H_0:\beta=0\)</span> against <span class="math inline">\(H_1:\beta\ne0\)</span>. The asterisks next to the p-values indicate the levels (e.g., <span class="math inline">\(\alpha=0.05\)</span>, <span class="math inline">\(\alpha=0.001\)</span>) for which we would conclude that the parameter is significantly different from zero. This test is naturally of interest in a regression setting because if <span class="math inline">\(\beta_2=0\)</span>, for example, then <span class="math inline">\(X_2\)</span> has no effect on the response <span class="math inline">\(Y\)</span>.</p>
<p><font size="4"><strong>Model Fit</strong></font></p>
<p>Now look at the last section where it says “Multiple R-squared: 0.601”. This value is the <span class="math inline">\(R^2\)</span> statistic, which measures the percent of the variation in <span class="math inline">\(Y\)</span> that is explained by the predictors. In this case, we find that 60.1% of the variation in <span class="math inline">\(Y\)</span> can be explained by <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. In general, it is difficult to define an absolute scale for what a “good” <span class="math inline">\(R^2\)</span> value is. In some contexts, 60% may be very high while in others it may be low. It likely depends on how difficult the response variable is to model and predict.</p>
<p><font size="4"><strong>Prediction</strong></font></p>
<p>Suppose I observed some new values of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, say <span class="math inline">\(X_1=0\)</span> and <span class="math inline">\(X_2=0.5\)</span>. How can I use the model to <strong>predict</strong> the corresponding value of <span class="math inline">\(Y\)</span>?</p>
<p>I could simply do the calculation by hand:
<span class="math display">\[\hat{Y}=\hat{\beta}_0 + \hat{\beta}_1X_1 + \hat{\beta}_2X_2 =-4.914 + 1.666(0) - 1.004(0.5)=-5.416\]</span>
where we use the “hat” notation to denote estimates or predicted values.</p>
<p>We can also use built-in prediction tools in R (where any differences would just be due to rounding error).</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb148-1" data-line-number="1"><span class="kw">predict</span>(lm.fit, <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">X1=</span><span class="dv">0</span>,<span class="dt">X2=</span><span class="fl">0.5</span>))</a></code></pre></div>
<pre><code>##      1 
## -5.415</code></pre>
<p>The first argument of the <code>predict( )</code> function is the regression object we created using the <code>lm( )</code> function. The second argument is the new set of covariates for which we want to predict a new response <span class="math inline">\(Y\)</span>. (Note: the names of variables in <code>newdata</code> must be the same names used in the original data.)</p>
<p><br></p>
</div>
<div id="regression-trees" class="section level2">
<h2><span class="header-section-number">6.2</span> Regression Trees</h2>
<hr>
<p>A natural question to ask now is what happens if the “true” model that generated our data was not linear? For example, our model could look something like this:</p>
<p><span class="math display">\[Y_i = \beta_0 + {\beta_1X_{1i} \over\beta_2 + X_{2i}} + \varepsilon_i\]</span></p>
<p>Here we still have three model parameters (<span class="math inline">\(\beta_0,\beta_1,\beta_2\)</span>), but they enter the regression function in a nonlinear fashion.</p>
<p>If we generate data from this model and then estimate the linear regression model from section 1, what will happen?</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb150-1" data-line-number="1"><span class="co"># generate data</span></a>
<a class="sourceLine" id="cb150-2" data-line-number="2">n =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb150-3" data-line-number="3">beta0 =<span class="st"> </span><span class="dv">-5</span></a>
<a class="sourceLine" id="cb150-4" data-line-number="4">beta1 =<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb150-5" data-line-number="5">beta2 =<span class="st"> </span><span class="dv">-1</span></a>
<a class="sourceLine" id="cb150-6" data-line-number="6">X1 =<span class="st"> </span><span class="kw">runif</span>(n,<span class="dt">min=</span><span class="op">-</span><span class="dv">1</span>,<span class="dt">max=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb150-7" data-line-number="7">X2 =<span class="st"> </span><span class="kw">runif</span>(n,<span class="dt">min=</span><span class="op">-</span><span class="dv">1</span>,<span class="dt">max=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb150-8" data-line-number="8">epsilon =<span class="st"> </span><span class="kw">rnorm</span>(n)</a>
<a class="sourceLine" id="cb150-9" data-line-number="9">Y =<span class="st"> </span>beta0 <span class="op">+</span><span class="st"> </span>beta1<span class="op">*</span>X1<span class="op">/</span>(beta2<span class="op">+</span>X2) <span class="op">+</span><span class="st"> </span>epsilon</a>
<a class="sourceLine" id="cb150-10" data-line-number="10"></a>
<a class="sourceLine" id="cb150-11" data-line-number="11"><span class="co"># estimate linear regression model</span></a>
<a class="sourceLine" id="cb150-12" data-line-number="12">lm.fit =<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X1 <span class="op">+</span><span class="st"> </span>X2)</a>
<a class="sourceLine" id="cb150-13" data-line-number="13"><span class="kw">summary</span>(lm.fit)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X1 + X2)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -47.93  -8.55  -1.96   4.48 165.50 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    -3.81       2.15   -1.77  0.07952 .  
## X1            -14.36       3.60   -3.99  0.00013 ***
## X2              5.73       3.51    1.63  0.10560    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 21.2 on 97 degrees of freedom
## Multiple R-squared:  0.17,   Adjusted R-squared:  0.153 
## F-statistic: 9.91 on 2 and 97 DF,  p-value: 0.000121</code></pre>
<p>The answer is that we get incorrect estimates of model parameters! (Remember <span class="math inline">\(\beta=-5,\beta_1=2,\beta_2=-1\)</span>.)</p>
<p>A more flexible approach to regression modeling is provided by <strong>regression trees</strong>. The idea is to split up the covariate space into homogeneous regions (with respect to the response <span class="math inline">\(Y\)</span>) and then fit simple linear models within each region.</p>
<p>We can use the <code>rpart</code> library in R to fit and plot regression trees. You’ll actually notice a similar syntax between <code>lm( )</code> and <code>rpart( )</code>.</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb152-1" data-line-number="1"><span class="kw">library</span>(rpart)</a>
<a class="sourceLine" id="cb152-2" data-line-number="2"></a>
<a class="sourceLine" id="cb152-3" data-line-number="3"><span class="co"># estimate regression tree</span></a>
<a class="sourceLine" id="cb152-4" data-line-number="4">tree.fit =<span class="st"> </span><span class="kw">rpart</span>(Y <span class="op">~</span><span class="st"> </span>X1 <span class="op">+</span><span class="st"> </span>X2)</a>
<a class="sourceLine" id="cb152-5" data-line-number="5"></a>
<a class="sourceLine" id="cb152-6" data-line-number="6"><span class="co"># plot the estimated tree</span></a>
<a class="sourceLine" id="cb152-7" data-line-number="7"><span class="kw">plot</span>(tree.fit, <span class="dt">uniform=</span><span class="ot">TRUE</span>, <span class="dt">margin=</span>.<span class="dv">05</span>)</a>
<a class="sourceLine" id="cb152-8" data-line-number="8"><span class="kw">text</span>(tree.fit)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-93-1.png" width="672" /></p>
<p>The output from a regression tree model looks very different from the output of a linear regression model. This is mostly because we had real-valued parameters in the linear model, but have much more complicated parameters in the tree model.</p>
<p>The top node is called the <strong>root node</strong> and indicates the most important variable for predicting <span class="math inline">\(Y\)</span>. Each subsequent node is called an <strong>interior node</strong> until you get to the last node showing a numeric value which is called a <strong>terminal node</strong>.</p>
<p>Tree models should be interpreted as a sequence of decisions for the purposes of making a prediction. Each node will present a logical statement and if that statement is true, we move <em>down and to the left</em> whereas if that statement is false, we move <em>down and to the right</em>.</p>
<p>For example, if you wanted to predict <span class="math inline">\(Y\)</span> when <span class="math inline">\(X_1=0\)</span> and <span class="math inline">\(X_2=0.5\)</span>, the root note first asks “Is <span class="math inline">\(X_1\geq -0.9113\)</span>?” If yes, then left and if no then right. Here our answer is yes, so we go to the next node to the left and ask “Is <span class="math inline">\(X_1\geq0.264\)</span>?” Our answer is no so we go to the right and ask <span class="math inline">\(X_2&lt;0.6835\)</span>?&quot; Our answer is yes so we go to the left. Since this represents the terminal node, we’re left with our predition of <span class="math inline">\(\hat{Y}=-4.037\)</span>. That is, if <span class="math inline">\(X_1=1\)</span> and <span class="math inline">\(X_2=9\)</span> then the model predicts <span class="math inline">\(\hat{Y}=-4.037\)</span>.</p>
<p>We can also use the <code>predict( )</code> function as we did with the linear regression model above.</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb153-1" data-line-number="1"><span class="kw">predict</span>(tree.fit, <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">X1=</span><span class="dv">0</span>,<span class="dt">X2=</span><span class="fl">0.5</span>))</a></code></pre></div>
<pre><code>##      1 
## -4.037</code></pre>
</div>
<div id="model-selection" class="section level2">
<h2><span class="header-section-number">6.3</span> Model Selection</h2>
<hr>
<p>Let’s see how regression trees compare to linear regression models in terms of out-of-sample prediction. We’ll consider two cases:</p>
<ol type="A">
<li>
The true model is a linear model
<li>
The true model is a nonlinear model
</ol>
<p><br>
<font size="4"> <strong>CASE A: TRUE MODEL IS LINEAR</strong> </font></p>
<p>First, we’ll generate a training and test data set from a linear regression model as in section 1. The training data set will be used for estimation and the test data will be used for prediction.</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb155-1" data-line-number="1">n =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb155-2" data-line-number="2">beta0 =<span class="st"> </span><span class="dv">-5</span></a>
<a class="sourceLine" id="cb155-3" data-line-number="3">beta1 =<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb155-4" data-line-number="4">beta2 =<span class="st"> </span><span class="dv">-1</span></a>
<a class="sourceLine" id="cb155-5" data-line-number="5">X1 =<span class="st"> </span><span class="kw">runif</span>(n,<span class="dt">min=</span><span class="op">-</span><span class="dv">1</span>,<span class="dt">max=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb155-6" data-line-number="6">X2 =<span class="st"> </span><span class="kw">runif</span>(n,<span class="dt">min=</span><span class="op">-</span><span class="dv">1</span>,<span class="dt">max=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb155-7" data-line-number="7">epsilon =<span class="st"> </span><span class="kw">rnorm</span>(n)</a>
<a class="sourceLine" id="cb155-8" data-line-number="8">Y =<span class="st"> </span>beta0 <span class="op">+</span><span class="st"> </span>beta1<span class="op">*</span>X1 <span class="op">+</span><span class="st"> </span>beta2<span class="op">*</span>X2 <span class="op">+</span><span class="st"> </span>epsilon</a>
<a class="sourceLine" id="cb155-9" data-line-number="9">train =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Y=</span>Y[<span class="dv">1</span><span class="op">:</span><span class="dv">70</span>], <span class="dt">X1=</span>X1[<span class="dv">1</span><span class="op">:</span><span class="dv">70</span>], <span class="dt">X2=</span>X2[<span class="dv">1</span><span class="op">:</span><span class="dv">70</span>])</a>
<a class="sourceLine" id="cb155-10" data-line-number="10">test =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Y=</span>Y[<span class="dv">71</span><span class="op">:</span><span class="dv">100</span>], <span class="dt">X1=</span>X1[<span class="dv">71</span><span class="op">:</span><span class="dv">100</span>], <span class="dt">X2=</span>X2[<span class="dv">71</span><span class="op">:</span><span class="dv">100</span>])</a></code></pre></div>
<p>Now let’s estimate both the linear regression and regression tree models on the training data.</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb156-1" data-line-number="1"><span class="co"># estimate linear regression model</span></a>
<a class="sourceLine" id="cb156-2" data-line-number="2">lm.fit =<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X1 <span class="op">+</span><span class="st"> </span>X2, <span class="dt">data=</span>train)</a>
<a class="sourceLine" id="cb156-3" data-line-number="3"></a>
<a class="sourceLine" id="cb156-4" data-line-number="4"><span class="co"># estimate regression tree model</span></a>
<a class="sourceLine" id="cb156-5" data-line-number="5">tree.fit =<span class="st"> </span><span class="kw">rpart</span>(Y <span class="op">~</span><span class="st"> </span>X1 <span class="op">+</span><span class="st"> </span>X2, <span class="dt">data=</span>train)</a></code></pre></div>
<p>To compare out-of-sample model performance, we’ll compute the root mean squared error (RMSE).
<span class="math display">\[\text{RMSE}=\sqrt{{1\over n}\sum_{i=1}^n (\hat{y}_i-y_i)^2}\]</span></p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb157-1" data-line-number="1"><span class="co"># linear regression model</span></a>
<a class="sourceLine" id="cb157-2" data-line-number="2">lm.predict =<span class="st"> </span><span class="kw">predict</span>(lm.fit, <span class="dt">newdata=</span>test)</a>
<a class="sourceLine" id="cb157-3" data-line-number="3">lm.rmse =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((lm.predict<span class="op">-</span>test<span class="op">$</span>Y)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb157-4" data-line-number="4">lm.rmse</a></code></pre></div>
<pre><code>## [1] 0.9157</code></pre>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb159-1" data-line-number="1"><span class="co"># regression tree model</span></a>
<a class="sourceLine" id="cb159-2" data-line-number="2">tree.predict =<span class="st"> </span><span class="kw">predict</span>(tree.fit, <span class="dt">newdata=</span>test)</a>
<a class="sourceLine" id="cb159-3" data-line-number="3">tree.rmse =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((tree.predict <span class="op">-</span><span class="st"> </span>test<span class="op">$</span>Y)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb159-4" data-line-number="4">tree.rmse</a></code></pre></div>
<pre><code>## [1] 1.149</code></pre>
<p>In this case the linear regression model has better predictive performance, which is not too surprising because we simulated the data from that model!</p>
<p><br>
<font size="4"> <strong>CASE B: TRUE MODEL IS NONLINEAR</strong> </font></p>
<p>We will again generate a training and test data set, but now from the nonlinear regression model we used in section 2.</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb161-1" data-line-number="1">n =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb161-2" data-line-number="2">beta0 =<span class="st"> </span><span class="dv">-5</span></a>
<a class="sourceLine" id="cb161-3" data-line-number="3">beta1 =<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb161-4" data-line-number="4">beta2 =<span class="st"> </span><span class="dv">-1</span></a>
<a class="sourceLine" id="cb161-5" data-line-number="5">X1 =<span class="st"> </span><span class="kw">runif</span>(n,<span class="dt">min=</span><span class="op">-</span><span class="dv">1</span>,<span class="dt">max=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb161-6" data-line-number="6">X2 =<span class="st"> </span><span class="kw">runif</span>(n,<span class="dt">min=</span><span class="op">-</span><span class="dv">1</span>,<span class="dt">max=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb161-7" data-line-number="7">epsilon =<span class="st"> </span><span class="kw">rnorm</span>(n)</a>
<a class="sourceLine" id="cb161-8" data-line-number="8">Y =<span class="st"> </span>beta0 <span class="op">+</span><span class="st"> </span>beta1<span class="op">*</span>X1<span class="op">/</span>(beta2<span class="op">+</span>X2) <span class="op">+</span><span class="st"> </span>epsilon</a>
<a class="sourceLine" id="cb161-9" data-line-number="9">train =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Y=</span>Y[<span class="dv">1</span><span class="op">:</span><span class="dv">70</span>], <span class="dt">X1=</span>X1[<span class="dv">1</span><span class="op">:</span><span class="dv">70</span>], <span class="dt">X2=</span>X2[<span class="dv">1</span><span class="op">:</span><span class="dv">70</span>])</a>
<a class="sourceLine" id="cb161-10" data-line-number="10">test =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Y=</span>Y[<span class="dv">71</span><span class="op">:</span><span class="dv">100</span>], <span class="dt">X1=</span>X1[<span class="dv">71</span><span class="op">:</span><span class="dv">100</span>], <span class="dt">X2=</span>X2[<span class="dv">71</span><span class="op">:</span><span class="dv">100</span>])</a></code></pre></div>
<p>Let’s again estimate both the linear regression and regression tree models on the training data and compute the predictive RMSE.</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb162-1" data-line-number="1"><span class="co"># linear regression model</span></a>
<a class="sourceLine" id="cb162-2" data-line-number="2">lm.fit =<span class="st"> </span><span class="kw">lm</span>(Y <span class="op">~</span><span class="st"> </span>X1 <span class="op">+</span><span class="st"> </span>X2, <span class="dt">data=</span>train)</a>
<a class="sourceLine" id="cb162-3" data-line-number="3">lm.predict =<span class="st"> </span><span class="kw">predict</span>(lm.fit, <span class="dt">newdata=</span>test)</a>
<a class="sourceLine" id="cb162-4" data-line-number="4">lm.rmse =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((lm.predict <span class="op">-</span><span class="st"> </span>test<span class="op">$</span>Y)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb162-5" data-line-number="5">lm.rmse</a></code></pre></div>
<pre><code>## [1] 18.76</code></pre>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb164-1" data-line-number="1"><span class="co"># regression tree model</span></a>
<a class="sourceLine" id="cb164-2" data-line-number="2">tree.fit =<span class="st"> </span><span class="kw">rpart</span>(Y <span class="op">~</span><span class="st"> </span>X1 <span class="op">+</span><span class="st"> </span>X2, <span class="dt">data=</span>train)</a>
<a class="sourceLine" id="cb164-3" data-line-number="3">tree.predict =<span class="st"> </span><span class="kw">predict</span>(tree.fit, <span class="dt">newdata=</span>test)</a>
<a class="sourceLine" id="cb164-4" data-line-number="4">tree.rmse =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">mean</span>((tree.predict <span class="op">-</span><span class="st"> </span>test<span class="op">$</span>Y)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb164-5" data-line-number="5">tree.rmse</a></code></pre></div>
<pre><code>## [1] 17.35</code></pre>
<p>Now the regression tree model has better predictive performance (but notice that the linear model still does relatively well!) In general, regression trees suffer from a problem called <strong>overfitting</strong>: the trees learn <em>too much</em> from the training data that they don’t generalize well to test data. There are ways of correcting for this, and you will learn more about them in Data Analyics II!</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="hypothesis-tests.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
